[{"content":"Until now Traditional CUDA programming has been based on CUDA C++. It requires mapping data to blocks and threads based on SIMT. While careful design can achieve optimal performance, it\u0026rsquo;s not something everyone can easily do. Additionally, as GPU architectures evolve, hardware specifications change, so kernel developers had to develop different optimal kernels for each GPU.\nIn response to these challenges, triton-language emerged (not related to NVIDIA Triton Inference Server). Triton improved accessibility through block-level abstractions such as memory management, synchronization, and tensor core scheduling. Furthermore, through MLIR, it has expanded as a means to connect to various hardware backends including not only NVIDIA GPUs but also AMD and NPUs.\nOf course, NVIDIA also provides libraries like CUTLASS to overcome these difficulties. By templating small tile-level operations, developers can relatively easily develop optimal kernels. FlashAttention-3 is also written based on CUTLASS. However, \u0026ldquo;relatively easy\u0026rdquo; is not truly easy. You still need to know numerous templates and logic. This also ultimately requires knowledge of CUDA C++.\nCUDA Tile IR Then came the announcement in spring 2025. News that NVIDIA is developing a tile-based programming model. I had been waiting for it to be released, and they released it before 2025 ended.\ncuTile remains at the same level as the existing SIMT-based programming model. This means that in addition to the existing path of CUDA C++ code development -\u0026gt; PTX -\u0026gt; CUBIN, a new path of Tile IR -\u0026gt; CUBIN has been added.\nTile IR is planned to be based on MLIR Dialect and will be stored as bytecode. Unless you\u0026rsquo;re developing parts that directly interact with Tile IR, you can use cuTile Python.\nNVIDIA cuTile Python: Most developers fall into this category. It\u0026rsquo;s a Python implementation that uses CUDA Tile IR as its backend. CUDA Tile IR: Developers who want to build their own DSL compilers or libraries use CUDA Tile IR. cuTile Python: how-to install The development environment is from the official documentation.\nLinux x86_64, Linux aarch64 or Windows x86_64 A GPU with compute capability 10.x or 12.x NVIDIA Driver r580 or later CUDA Toolkit 13.1 or later Python version 3.10, 3.11, 3.12 or 3.13 Once the environment is set up, install via pip. Install torch according to your environment as well.\npip install cuda-tile pip install cupy-cuda13x # For sample pip install pytest numpy # For test Key concepts: Tile vs. SIMT The tile model (left) partitions data into blocks and the compiler maps them to threads. The SIMT model (right) maps data to both blocks and threads. SIMT allows control of each individual thread, but achieving optimal performance requires manual tuning that considers hardware complexity. The tile model abstracts some of the hardware complexity, allowing the CUDA compiler/runtime to handle tile algorithms internally, while users can focus on algorithm development.\nKernel Definition The @ct.kernel decorator compiles a Python function into a GPU kernel. As we learned above, the content of this kernel doesn\u0026rsquo;t generate PTX through CUDA C++, but rather goes down to Tile IR and generates cubin through MLIR using CUDA TILE IR.\nimport cuda.tile as ct @ct.kernel def vector_add(a, b, c, tile_size: ct.Constant[int]): ... class kernel(TileDispatcher): constant_flags = get_constant_arg_flags(function) compiler_options = CompilerOptions( num_ctas=num_ctas, occupancy=occupancy, opt_level=opt_level ) compile = _compile.CompileCallback(function, compiler_options) super().__init__(constant_flags, compile) self._pyfunc = function Tutorial w/ vector addition Let\u0026rsquo;s learn how to use it through vector addition kernel, the most basic CUDA programming example.\n__global__ void vecAdd(float* A, float* B, float* C, int vectorLength) { /* calculate my thread index */ int workIndex = threadIdx.x + blockIdx.x*blockDim.x; if(workIndex \u0026lt; vectorLength) { /* perform the vector addition */ C[workIndex] = A[workIndex] + B[workIndex]; } } 1D Tile This is the simplest form of a cuTile kernel.\nLoad one or more tiles from GPU memory Perform computation on tiles to produce new tiles Store result tiles to GPU memory import cuda.tile as ct @ct.kernel def vector_add(a, b, c, tile_size: ct.Constant[int]): # Get the 1D pid, blockIdx.x in cuda c++ pid = ct.bid(0) # Load input tiles a_tile = ct.load(a, index=(pid,) , shape=(tile_size, ) ) b_tile = ct.load(b, index=(pid,) , shape=(tile_size, ) ) # Perform elementwise addition result = a_tile + b_tile # Store result ct.store(c, index=(pid, ), tile=result) ct.bid(0) gets the block ID along axis-0. It\u0026rsquo;s equivalent to getting blockIdx in CUDA C++. ct.load() loads data from memory at the required index and tile shape. Adding the loaded a_tile and b_tile is done with the + operator, and the result is held in the result tile, which hasn\u0026rsquo;t been stored to DRAM yet, so we store it using ct.store().\n2D Tile Reshaping a 1D tile to 2D makes it suitable for matrix operations. Indexing is truly intuitive as follows.\n@ct.kernel def vec_add_kernel_2d(a, b, c, TILE_X: ct.Constant[int], TILE_Y: ct.Constant[int]): # Get the global IDs of the current block along the X and Y axes. # `ct.bid(0)` for the first grid dimension (typically rows), # `ct.bid(1)` for the second grid dimension (typically columns). bid_x = ct.bid(0) bid_y = ct.bid(1) # Load `TILE_X` x `TILE_Y` chunks from input matrices \u0026#39;a\u0026#39; and \u0026#39;b\u0026#39;. # The `index=(bid_x, bid_y)` specifies the 2D tile to load. a_tile = ct.load(a, index=(bid_x, bid_y), shape=(TILE_X, TILE_Y)) b_tile = ct.load(b, index=(bid_x, bid_y), shape=(TILE_X, TILE_Y)) # Perform the element-wise addition on the loaded tiles. sum_tile = a_tile + b_tile # Store the resulting `TILE_X` x `TILE_Y` chunk back to the output matrix \u0026#39;c\u0026#39;. ct.store(c, index=(bid_x, bid_y), tile=sum_tile) When tiles don\u0026rsquo;t fit The two examples above are cases where the total data size divides evenly by the tile size. For cases that don\u0026rsquo;t align with tiles, use ct.gather() and ct.scatter(). This is also recommended for non-power-of-two cases. When out of bounds, padding_value (default: 0) is returned.\n@ct.kernel def vec_add_kernel_2d_gather( a, b, c, TILE_X: ConstInt, TILE_Y: ConstInt # Tile dimensions for this block ): # Get the global IDs of the current block along the X and Y axes. bid_x = ct.bid(0) bid_y = ct.bid(1) # Calculate X and Y indices within the current block\u0026#39;s tile. x = bid_x * TILE_X + ct.arange(TILE_X, dtype=torch.int32) y = bid_y * TILE_Y + ct.arange(TILE_Y, dtype=torch.int32) # Reshape the X and Y indices to (TILE_X, 1) and (1, TILE_Y), respectively. # This way, they can be broadcasted together to a common shape (TILE_X, TILE_Y). x = x[:, None] y = y[None, :] # Load elements using the calculated X and Y indices. # Both `a_tile` and `b_tile` have shape (TILE_X, TILE_Y). a_tile = ct.gather(a, (x, y)) b_tile = ct.gather(b, (x, y)) # Perform the element-wise addition. sum_tile = a_tile + b_tile # Store the result back to `c` using the same index tiles. # `ct.scatter()` only writes data to positions within the array bounds. ct.scatter(c, (x, y), sum_tile) Performance Running cutile-python/test/bench_matmul.py gives the following results. The Torch kernel internally calls the cutlass kernel. While performance is still slower compared to torch, it\u0026rsquo;s promising that tile-based programming has become possible, significantly reducing kernel implementation difficulty. Since the IR has also been released as open source, performance will gradually improve going forward.\n--------------------------------------------------------------------------------------------------- benchmark \u0026#39;matmul\u0026#39;: 12 tests --------------------------------------------------------------------------------------------------- Name (time in us) Min Max Mean StdDev Median IQR Outliers OPS Rounds Iterations ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ bench_matmul[1K-1K-1K-f16-torch] 14.3439 (1.0) 14.3451 (1.0) 14.3443 (1.0) 0.0005 (1.0) 14.3441 (1.0) 0.0005 (1.0) 1;0 69,714.0067 (1.0) 5 5244 bench_matmul[1K-1K-1K-f32-torch] 26.6370 (1.86) 26.6386 (1.86) 26.6374 (1.86) 0.0006 (1.37) 26.6372 (1.86) 0.0005 (1.02) 1;1 37,541.1889 (0.54) 5 3218 bench_matmul[1K-1K-1K-f16-cutile] 154.3699 (10.76) 154.4450 (10.77) 154.4040 (10.76) 0.0277 (58.67) 154.4062 (10.76) 0.0332 (73.27) 2;0 6,476.5145 (0.09) 5 613 bench_matmul[1K-1K-1K-f32-cutile] 892.8400 (62.25) 893.2511 (62.27) 893.0082 (62.26) 0.1616 (342.04) 893.0112 (62.26) 0.2286 (505.17) 2;0 1,119.8106 (0.02) 5 112 References https://docs.nvidia.com/cuda/cutile-python/ https://developer.nvidia.com/blog/focus-on-your-algorithm-nvidia-cuda-tile-handles-the-hardware https://developer.nvidia.com/blog/simplify-gpu-programming-with-nvidia-cuda-tile-in-python ","date":"2026-01-12T00:00:00Z","permalink":"https://jinseok-moon.github.io/p/cuda-tile/","title":"CUDA Tile IR, cuTile Python"},{"content":"GPU architectures have evolved over many generations and their features have changed accordingly. If you work with LLMs, it is especially useful to understand the hardware characteristics starting from Ampere. In this post we will look at representative GPUs for each architecture—A100, H100 and Blackwell—without focusing on raw numbers like TOPS from Tensor Cores.\nA100: Ampere (SM80, 2020) Released in 2020, A100 introduced an L1‑bypass path that optimizes copies from DRAM to shared memory (SRAM) by skipping several intermediate stages.\nThese copies are implemented by the cp.async PTX instructions and are asynchronous, so we can hide their latency with a software pipeline like the one below.\nSoftware pipelining is a technique that removes dependency chains between successive instructions to better utilize hardware. Memory instructions are handled by the LSU (Load/Store Unit), while matrix multiplications are handled by the compute units (Tensor Cores), so there is no structural hazard between them. However, the following data dependency can still be a problem:\nfor (i=0; i\u0026lt;N-1; i++) { load_to_register(i); compute(i); } We cannot execute compute(i) until load_to_register(i) has finished, because compute(i) needs the loaded data. To break this dependency chain, we restructure the loop into a pipeline:\nload_to_register(0); for (i=0; i\u0026lt;N-2; i++) { load_to_register(i+1); compute(i); } compute(N-1); Now the dependency between the load and compute for the same iteration is removed, allowing the hardware to overlap them. FlashAttention-2 is a good example of a kernel that fully exploits this pattern; according to the authors, it approaches the theoretical peak performance on Ampere. Another heavily optimized kernel in this spirit is the mixed‑precision GEMM kernel Marlin (which I cover in a separate post).\nH100: Hopper (SM90, 2022) Hopper was announced at GTC 2022 and delivers substantially higher performance than Ampere. The key architectural features we care about are TMA, WGMMA and warp specialization. Kernels such as FlashAttention-3 and Machete are good examples of code that fully exploits these capabilities.\nTensor Memory Accelerator (TMA) On Ampere, L1‑bypass improved the performance of memory copies, but programmers still had to compute addresses and strides manually and manage synchronization barriers. To further reduce that burden, NVIDIA introduced the Tensor Memory Accelerator (TMA). With TMA you describe multi‑dimensional tensor layouts and the hardware performs bulk copies accordingly. TMA instructions are launched from a single thread, making much more efficient use of resources.\nWarp Group Matrix Multiply-Accumulate (WGMMA) Up through Ampere, MMA instructions were warp‑local. Hopper goes a step further: WGMMA groups 4 warps together to execute a single matrix multiply‑accumulate instruction, driving the Tensor Cores harder and improving throughput.\nWarp Specialization: Consumer-Producer Warp specialization is a technique that makes direct use of Hopper’s ability to allocate different numbers of registers to different warps. Conceptually, our pipeline has two main stages: (1) memory and (2) computation. Memory operations need relatively few registers, while compute-heavy WGMMA operations benefit from many.\nProducer warp group: issues TMA memory instructions, requires few registers, focuses on pulling data in. Consumer warp group: runs WGMMA on Tensor Cores, uses many registers, focuses on computation. Blackwell (SM100, 2024) Blackwell, announced at GTC 2024, is the next‑generation GPU architecture. Once again, a lot changed. Most notably, WGMMA is gone—it was effectively a Hopper‑only instruction. Instead, Blackwell introduces UMMA, with the following operand constraints:\nOperand A: TMEM or SMEM Operand B: SMEM Accumulator: TMEM Tensor Memory (TMEM) is a new storage structure introduced in Blackwell. Having the accumulator live in TMEM means that UMMA does not need regular registers for its accumulation. In other words, UMMA can run as a single‑thread instruction without consuming the usual register budget. Combined with TMA, most of the heavy lifting happens in specialized hardware; the CTA (Cooperative Thread Array, i.e., CUDA block) mainly handles setup and post‑processing.\nHistorically, all of these changes follow the same trend: offloading more work to specialized hardware so that general‑purpose resources are freed for other tasks.\nVolta: separated matrix math onto Tensor Cores, offloading it from the regular pipelines Ampere: enabled true pipelining by overlapping async copies with compute Hopper: used TMA and WGMMA to overlap data movement and MMA at low overhead Blackwell: uses TMEM and UMMA so that even MMA can run as a single‑thread, asynchronous operation without consuming many registers Grace-Blackwell GB200 Grace–Blackwell combines NVIDIA’s Grace CPU with a Blackwell GPU. Everything we discussed above happens inside the GPU, but we still need to move data from host (CPU) to device (GPU). Traditionally this went over PCIe, which is relatively slow. Grace–Blackwell instead connects CPU and GPU with NVLink, which is much faster and better suited for large‑scale LLM workloads.\nBlackwell GeForce RTX 50 series (SM120, 2025) These are the consumer GeForce GPUs based on Blackwell. Prices are high (though they are slowly coming down). Architecturally they are Blackwell, but they do not expose TMEM, even though they keep TMA. For LLM workloads you can still build Ampere‑style pipelines enhanced with TMA, but TMEM‑centric designs from server GPUs do not transfer directly. On the other hand, Tensor Core performance is dramatically better than previous GeForce generations.\nReferences CUTLASS Tutorial: Efficient GEMM kernel designs with Pipelining CUTLASS Tutorial: Writing GEMM Kernels Using Tensor Memory For NVIDIA® Blackwell GPUs ","date":"2025-11-24T00:00:00Z","permalink":"https://jinseok-moon.github.io/p/archs/","title":"GPU architecture w/ LLM"},{"content":"Arithmetic Intensity (AI) Arithmetic intensity (AI) is defined as the ratio of operations to memory traffic, typically measured in ops/byte. A higher AI means you can perform more computation per byte of data moved. In the previous chapter we used CUDA shared memory (SRAM) and 1D tiling to improve performance, letting each thread compute multiple output elements as shown below. Let’s revisit that setup and think about how AI changes as we extend the algorithm.\nIn the original kernel where each thread produced just one result, we needed 17 loads per output. With 1D tiling, that dropped to 11 loads. Moving to 2D tiling reduces it further to 9 loads. This reflects a fundamental property of GEMM: we can dramatically improve efficiency by reusing data in multiple output elements.\n4. SRAM 2d tiling Since 2D tiling is more effective, let’s implement it. We introduce a new parameter TN and extend the loops accordingly.\nint totalResultsBlocktile = BM * BN; // 128*128=16384 int numThreadsBlocktile = totalResultsBlocktile / (TM * TN); // 16384/(8*8)=256 int strideA = numThreadsBlocktile / BK; // 256/8=32 for (int bkIdx = 0; bkIdx \u0026lt; K; bkIdx += BK) { for (int offset = 0; offset \u0026lt; BM; offset += strideA) { A_shared[(innerRowA + offset) * BK + innerColA] = A[(innerRowA + offset) * K + innerColA]; } for (int offset = 0; offset \u0026lt; BK; offset += strideB) { B_shared[(innerRowB + offset) * BN + innerColB] = B[(innerRowB + offset) * N + innerColB]; } __syncthreads(); A += BK; B += BK * N; for (int dotIdx = 0; dotIdx \u0026lt; BK; dotIdx++) { for (int i = 0; i \u0026lt; TM; i++) { regM[i] = A_shared[(threadRow * TM + i) * BK + dotIdx]; } for (int i = 0; i \u0026lt; TN; i++) { regN[i] = B_shared[dotIdx * BN + threadCol * TN + i]; } for (int resIdxM = 0; resIdxM \u0026lt; TM; resIdxM++) { for (int resIdxN = 0; resIdxN \u0026lt; TN; resIdxN++) { threadResults[resIdxM * TN + resIdxN] += regM[resIdxM] * regN[resIdxN]; } } } __syncthreads(); } We launch the kernel with BM = BN = 128 and BK = TM = TN = 8, which yields 256 threads per block:\ntemplate \u0026lt;int BM, int BN, int BK, int TM, int TN\u0026gt; void launch_gpu_kernel_4(float *A, float *B, float *C, int M, int N, int K) { dim3 block((BM * BN) / (TM * TN)); dim3 grid(ceil_div(N, BN), ceil_div(M, BM)); gemm_gpu_4_sram_2d_tiling\u0026lt;BM, BN, BK, TM, TN\u0026gt; \u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(A, B, C, M, N, K); } If we conceptually unroll the dotIdx loop, the access pattern looks like the figure above. In total, we only need 16 shared‑memory loads along this path.\nDRAM: K/8 iters * 2 (=A+B) * 4 (=sizeSRAM/numThreads) loads SRAM: K/8 iters * 8 (=dotIdx) * 2 (=A+B) * 8 (=TM,=TN) loads Memory accesses per result: K/64 DRAM, K/4 SRAM 5. Vectorized SRAM 2d tiling On NVIDIA GPUs, the shared‑memory load instruction LDS can handle up to 128 bits at a time. This means we can read more data per instruction if we transpose $A$ in the 2D‑tiling kernel so that we can use vectorized loads. By transposing $A$ during the global‑to‑shared copy, we can use LDS.128 in the same way we already do for $B$.\nBy using the float4 vector type, the compiler generates 128‑bit load instructions, which improves performance.\nfloat4 tmp = reinterpret_cast\u0026lt;float4 *\u0026gt;(\u0026amp;A[innerRowA * K + innerColA * 4])[0]; // transpose A during the GMEM to SMEM transfer As[(innerColA * 4 + 0) * BM + innerRowA] = tmp.x; As[(innerColA * 4 + 1) * BM + innerRowA] = tmp.y; As[(innerColA * 4 + 2) * BM + innerRowA] = tmp.z; As[(innerColA * 4 + 3) * BM + innerRowA] = tmp.w; reinterpret_cast\u0026lt;float4 *\u0026gt;(\u0026amp;Bs[innerRowB * BN + innerColB * 4])[0] = reinterpret_cast\u0026lt;float4 *\u0026gt;(\u0026amp;B[innerRowB * N + innerColB * 4])[0]; __syncthreads(); ","date":"2025-11-08T00:00:00Z","permalink":"https://jinseok-moon.github.io/p/gemm-2/","title":"GEMM 2"},{"content":"This post is my study notes based on the excellent worklog. I rewrote the kernels and diagrams myself while following along.\nCUDA provides highly optimized GEMM APIs in cuBLAS, but with enough care we can write custom kernels that approach cuBLAS performance. Below, we will gradually apply core CUDA optimization ideas to a simple GEMM kernel.\nA: (M, K), row-major B: (K, N), row-major C: (M, N), row-major DRAM: Global memory SRAM: Shared memory We implement a series of kernels and compare their performance. The results are:\nNaive implementation, DRAM coalescing SRAM caching SRAM 1d tiling [BENCHMARK] CUBLAS GEMM │ 0.045334 ms (w:10 r:20) [BENCHMARK] GPU GEMM 0 NAIVE │ 3.943722 ms (w:10 r:20) [PASSED] [BENCHMARK] GPU GEMM 0 DRAM COALESCING │ 0.517949 ms (w:10 r:20) [PASSED] [BENCHMARK] GPU GEMM 1 SRAM CACHING │ 0.248670 ms (w:10 r:20) [PASSED] [BENCHMARK] GPU GEMM 2 SRAM 1D TILING │ 0.249046 ms (w:10 r:20) [PASSED] 0. Naive implementation The most basic implementation assigns one element of $C$ to each thread. Because the matrices are stored in row-major order, this layout forces us to read non-contiguous memory for $B$. The following figure shows how this looks at the warp level.\nIn this loop structure, when loading $A$ each thread in a warp accesses a different column, so the accesses are not contiguous and cannot be coalesced. Without coalescing, the hardware effectively has to service 32 separate loads per warp, which significantly degrades performance.\nWhen loading $B$, all threads in the warp access the same element, so the hardware can use a broadcast. Even so, given the overall access pattern, there is little benefit from grouping these threads into a single warp in this naïve mapping.\nDRAM coalescing To leverage the warp properly, we need to access contiguous memory as shown above. When loading $A$, we can exploit broadcast to share values within the warp. When loading $B$, each thread in the warp accesses adjacent elements, so the hardware can coalesce the loads into 128‑byte transactions.\n__global__ void gemm_gpu_0_naive(int M, int N, int K, float alpha, float *A, float *B, float beta, float *C) { int tid = blockIdx.x * blockDim.x + threadIdx.x; int row = tid % N; int col = tid / N; ... } __global__ void gemm_gpu_0_dram_coalescing(int M, int N, int K, float alpha, float *A, float *B, float beta, float *C) { int tid = blockIdx.x * blockDim.x + threadIdx.x; int row = tid / N; int col = tid % N; ... } The only difference between the two kernels is how row and col are computed, yet the performance difference is large. Profiling shows that this gap comes from DRAM operations.\n# RTX 5090 $ sudo /usr/local/cuda/bin/ncu --metrics dram__bytes.sum.per_second gemm gemm_gpu_0_naive(int, int, int, float, float *, float *, float, float *) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 12.0 Section: Command line profiler metrics -------------------------- ----------- ------------ Metric Name Metric Unit Metric Value -------------------------- ----------- ------------ dram__bytes.sum.per_second Gbyte/s 2.40 -------------------------- ----------- ------------ gemm_gpu_0_dram_coalescing(int, int, int, float, float *, float *, float, float *) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 12.0 Section: Command line profiler metrics -------------------------- ----------- ------------ Metric Name Metric Unit Metric Value -------------------------- ----------- ------------ dram__bytes.sum.per_second Gbyte/s 18.07 -------------------------- ----------- ------------ 1. SRAM caching In the naïve kernel, the same data is fetched from DRAM many times, which is very expensive. According to this paper, on a V100 the DRAM bandwidth is about 900 GB/s, while the shared-memory (SRAM) bandwidth is around 13,800 GB/s (the exact SRAM number is not officially documented). We therefore want to cache data in shared memory and reuse it as much as possible.\nEach block is responsible for a 32×32 tile of $C$. The shaded regions in the figure indicate the corresponding tiles of $A$ and $B$ that must be loaded from DRAM. The bkIdx loop walks along $K$ in chunks of BLOCKSIZE, loading tiles into shared memory; the tIter loop then performs the GEMM on those tiles. Since each thread produces a single output element, it accumulates the partial result into a scalar sum, which is finally written back to the appropriate location in $C$.\ntemplate \u0026lt;int BLOCKSIZE\u0026gt; __global__ void gemm_gpu_1_sram_caching(int M, int N, int K, float alpha, float *A, float *B, float beta, float *C) { int bkRow = blockIdx.y; int bkCol = blockIdx.x; A += K * BLOCKSIZE * bkRow; B += BLOCKSIZE * bkCol; C += N * BLOCKSIZE * bkRow + BLOCKSIZE * bkCol; __shared__ float sA[BLOCKSIZE * BLOCKSIZE]; __shared__ float sB[BLOCKSIZE * BLOCKSIZE]; int tRow = threadIdx.x / BLOCKSIZE; int tCol = threadIdx.x % BLOCKSIZE; float sum = 0.0f; for (int bkIdx = 0; bkIdx \u0026lt; K; bkIdx += BLOCKSIZE) { sA[threadIdx.x] = A[tRow * K + tCol]; sB[threadIdx.x] = B[tRow * N + tCol]; __syncthreads(); A += BLOCKSIZE; B += BLOCKSIZE * N; for (int tIter = 0; tIter \u0026lt; BLOCKSIZE; tIter++) { sum += sA[tRow * BLOCKSIZE + tIter] * sB[tIter * BLOCKSIZE + tCol]; } __syncthreads(); } C[tRow * N + tCol] = alpha * sum + beta * C[tRow * N + tCol]; } 2. SRAM 1D tiling SRAM caching significantly improves performance, but it is still not on par with cuBLAS. In the current design, each thread produces one output element. Its memory access pattern looks like this: for each element of $C$, we need approximately K/16 DRAM loads and K*2 shared-memory loads.\nDRAM: K/32 iterations of outer loop * 2 loads SRAM: K/32 iterations of outer loop * BLOCKSIZE (=32) * 2 loads Memory accesses per result: K / 16 DRAM, K * 2 SRAM Profiling shows that warps often stall on memory input/output (MIO), confirming that memory traffic is the main bottleneck.\nWe can alleviate this by reusing each loaded value more aggressively. If each thread computes 8 output elements instead of just 1 (a 1D tiling in the $M$ dimension), the access pattern becomes:\nWithin a warp, each thread now computes 8 elements of $C$ along the column direction. Recomputing the memory accesses per result under this scheme gives us:\nDRAM: K/8 iters (dotIdx) loop * 2 loads SRAM: K/8 iters (dotIdx) loop * BK(=8) * (1 + TM(=8)) Memory accesses per result: K/32 DRAM, K * 9/8 SRAM This reduces the number of memory accesses per output element from K/16 to K/32 for DRAM and from K*2 to K*9/8 for shared memory.\nfor (int bkIdx = 0; bkIdx \u0026lt; K; bkIdx += BK) { sA[innerRowA * BK + innerColA] = A[innerRowA * K + innerColA]; sB[innerRowB * BN + innerColB] = B[innerRowB * N + innerRowB]; __syncthreads(); A += BK; B += BK * N; for (int dotIdx = 0; dotIdx \u0026lt; BK; dotIdx++) { float _b = sB[dotIdx * BN + tCol]; for (int resIdx = 0; resIdx \u0026lt; TM; resIdx++) { sum[resIdx] += sA[(tRow * TM + resIdx) * BK + dotIdx] * _b; } } __syncthreads(); } In this kernel, BM and TM must be chosen so that BM == TM * (number of results per thread in M). The number of threads per block is (BM * BN / TM)and we rely on that to match the sizes of sA and sB so that the DRAM→SRAM loads can be implemented with simple strided accesses.\nsA[innerRowA * BK + innerColA] = A[innerRowA * K + innerColA]; sB[innerRowB * BN + innerColB] = B[innerRowB * N + innerRowB]; References How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog ","date":"2025-10-26T00:00:00Z","permalink":"https://jinseok-moon.github.io/p/gemm-1/","title":"GEMM 1"},{"content":"CUDA exposes a logical hierarchy of grid → block → thread, as shown in the figure below. Threads inside a block can be arranged in three dimensions. The key takeaway is: unless you have a very specific memory layout, compute the linear thread index from (x, y, z) as $(x + y \\cdot D_x + z \\cdot D_x \\cdot D_y)$. Let’s see why.\nDoes it matter if we arbitrarily swap the x, yand z dimensions when mapping to memory? The answer is no, you cannot ignore it. According to the CUDA Programming Guide, the linear thread ID for a 3D block is computed as follows:\n(image-1.png)\nLet’s see what happens if we ignore this in a real kernel. Suppose we have the following two kernels. Each adds two vectors of length 1024 and stores the result in output — this is a simple \u0026ldquo;2 loads + 1 store\u0026rdquo; pattern.\nkernel_0: index computed in x–y order kernel_1: index computed in y–x order __global__ void kernel_0(int* d_vec_a, int* d_vec_b, int* output, int size) { int index = threadIdx.x * blockDim.y + threadIdx.y; output[index] = d_vec_a[index] + d_vec_b[index]; } __global__ void kernel_0(int* d_vec_a, int* d_vec_b, int* output, int size) { int index = threadIdx.y * blockDim.x + threadIdx.x; output[index] = d_vec_a[index] + d_vec_b[index]; } Because each logical thread is mapped to the correct memory location, both kernels produce the same numerical result. However, their performance differs: kernel_0 is noticeably slower.\nkernel_0 mean time: 0.0052768 ms kernel_1 mean time: 0.0028352 ms The reason is that CUDA executes instructions at the warp level. When a warp issues a memory load, the hardware tries to fetch a contiguous 128‑byte segment from DRAM. In kernel_0, each thread in the warp ends up accessing data far apart in memory, so most of that 128‑byte segment is unused — this is similar to reading a row‑major matrix using column‑major indices. This uncoalesced access pattern severely hurts performance.\n","date":"2025-07-25T00:00:00Z","permalink":"https://jinseok-moon.github.io/p/thread-indexing/","title":"Proper thread indexing and memory coalescing"},{"content":"Original softmax $$ \\sigma_i(\\mathbf{z}) = \\frac{e^{z_i}}{\\sum^K_{j=1}e^{z_j}} $$\nThe formula for softmax is shown above. For an input vector, a naïve implementation requires two loads and one store, for a total of three memory accesses per element. Softmax converts raw scores into a probability distribution over the input values. It is extremely useful, but in practice it is numerically fragile due to floating‑point limitations. Floating point represents real numbers within a finite dynamic range. Because softmax uses the exponential function $e^z$, the values can grow very largeand the sum in the denominator is prone to overflow. In the opposite direction, for large negative inputs $e^z$ becomes very close to zeroand the denominator can underflow toward 0, leading to undefined behavior when used in division.\nSafe softmax To address this, we can rewrite the formula by subtracting the maximum value from every element. After this shift, all elements are less than or equal to zero and the dynamic range is much narrower. In the denominator, one of the terms becomes $e^0 = 1$ (the term corresponding to the maximum element), which guarantees that the denominator cannot be exactly zero.\n$$ \\sigma_i(\\mathbf{z}) = \\frac{e^{z_i - \\max(z)}}{\\sum^K_{j=1}e^{z_j-\\max(z)}} $$\nThis \u0026ldquo;safe\u0026rdquo; softmax is mathematically equivalent to the original definition but avoids overflow and underflow in practice, so it is the version that most implementations use. The downside is that we now need an extra pass to compute the maximum $m_k$, so the number of memory accesses increases to four.\nOnline softmax calculation Online softmax is a way to reduce the number of memory accesses again. If you look closely at the safe softmax derivation, you will notice that we do not strictly need the global maximum of the vector; we just need a running value that is large enough to keep the exponentials well behaved. We can maintain such a value using local maxima as we stream through the data.\nThe denominator $d_V$ in the algorithm above can be expressed recursively. Using the exponential rules, we can factor out the terms of the form $e^m$.\nThis derivation shows how to update the denominator $d_V$ on the fly while tracking only a local maximum. Is this still numerically safe? Yes: for each step $j$, the stabilizing term $m_j$ is chosen large enough that $x_i - m_j \\le 0$ for all processed elements, so $e^{x_i - m_j} \\le 1$. As a result, $d_j$ stays within a well‑behaved range between 1 and $j$, preventing overflow or underflow.\nReferences Online normalizer calculation for softmax ","date":"2025-07-12T00:00:00Z","permalink":"https://jinseok-moon.github.io/p/online-softmax/","title":"Online softmax"},{"content":"First, we define a Triton kernel in Python using the Triton language. A Triton kernel is declared with the @triton.jit decorator. (The full Triton compilation pipeline will be covered in another post.)\n@triton.jit def _fwd_kernel( Q, K, V, Out, Lse, TMP, softmax_scale, batch, nheads, ... , EVEN_M, EVEN_N, EVEN_HEADDIM, IS_CAUSAL: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, ): Once the function is defined, we can invoke the Triton compiler.\nexport TRITON_ROOT=$(pip show triton | grep Location | cut -d\u0026#39; \u0026#39; -f2) rm -rf aot mkdir -p aot/fp16 python ${TRITON_ROOT}/triton/tools/compile.py \\ fmha_triton.py \\ -n _fwd_kernel \\ -o aot/fp16/fmha_kernel_d64_fp16 \\ --out-name fmha_d64_fp16 \\ -w 4 \\ -ns 1 \\ -s \u0026#34;*fp16, *fp16, *fp16, *fp16, *fp32, *fp32, fp32, \\ i32, i32, i32, \\ i32, i32, i32, \\ i32, i32, i32, \\ i32, i32, i32, \\ i32, i32, i32, \\ i32, i32, i32, \\ i1, i1, i1, \\ 1, \\ 64, \\ 128, \\ 128\u0026#34; \\ -g \u0026#34;(seqlen_q + 127) / 128, batch * nheads, 1\u0026#34; -n: name of the Triton kernel defined in Python -o: output directory/path \u0026ndash;out-name: function name that will be visible from C++ -w: number of warps -ns: number of pipeline stages -s: function signature (parameter data types) -g: CUDA grid configuration; you can use runtime parameters such as batch and nheads Finally, run python ${TRITON_ROOT}/triton/tools/link.py aot/fp16/*.h -o aot/fmha_kernel_fp16 to link the generated pieces. This produces the following files:\naot ├── fmha_kernel_fp16.c ├── fmha_kernel_fp16.h └── fp16 ├── fmha_kernel_d64_fp16.6979ce4b_0123456789101112131415161718192021222324252627.c └── fmha_kernel_d64_fp16.6979ce4b_0123456789101112131415161718192021222324252627.h In your actual C++ source, include these files and declare the generated kernel with extern \u0026quot;C\u0026quot; to avoid name mangling. The wrapper inside uses the CUDA Driver API under the hood.\nres = fmha_d64_fp16_default(stream, reinterpret_cast\u0026lt;CUdeviceptr\u0026gt;(Q), reinterpret_cast\u0026lt;CUdeviceptr\u0026gt;(K), reinterpret_cast\u0026lt;CUdeviceptr\u0026gt;(V), reinterpret_cast\u0026lt;CUdeviceptr\u0026gt;(output), reinterpret_cast\u0026lt;CUdeviceptr\u0026gt;(LSE), reinterpret_cast\u0026lt;CUdeviceptr\u0026gt;(TMP), mscale, head_num * head_dim * seq_len, head_dim, head_dim*head_num, head_num * head_dim * seq_len, head_dim, head_dim*head_num, head_num * head_dim * seq_len, head_dim, head_dim*head_num, head_num * head_dim * seq_len, head_dim, head_dim*head_num, head_num, seq_len, seq_len, seqlen_q_rounded, head_dim, batch_size, even_m, even_n, even_headdim); You simply need to wire up the parameters correctly and launch the kernel as shown.\n","date":"2025-07-05T00:00:00Z","permalink":"https://jinseok-moon.github.io/p/triton-kernel-link/","title":"Triton kernel linking with CUDA C++"},{"content":"In CUDA, one common way to copy memory from the host to the device is via the cudaMemcpy API. By default, memory you allocate on the host without any special handling is pageable memory. To copy data from pageable host memory to the device, the driver first has to move it into an internal pinned buffer, which introduces extra overhead and slows the transfer down.\nIf you explicitly allocate host memory with cudaMallocHost, you get pinned (page-locked) memory directly. In that case, the intermediate copy step is skippedand transfers can proceed faster. For example, when copying 1 GB of memory to the device, the performance difference between the two approaches looks like this:\n$ ./pinned_memory Pinned memory Total time: 185.875 ms Average time per copy: 18.5875 ms Data size: 1 GB Bandwidth: 53.7997 GB/s Pageable memory Total time: 367.491 ms Average time per copy: 36.7491 ms Data size: 1 GB Bandwidth: 27.2116 GB/s We can see that using pinned memory significantly improves throughput. However, pinned memory consumes physical system RAM and cannot be paged out, so it should be used selectively and only where it makes sense.\nReferences https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/ ","date":"2025-06-19T00:00:00Z","permalink":"https://jinseok-moon.github.io/p/pinned-memory/","title":"Pageable vs. pinned data transfer"},{"content":"Bank Conflicts When you study CUDA, you naturally end up studying shared memoryand along the way you will encounter bank conflicts. A bank conflict occurs when multiple memory requests from a warp are mapped to the same memory bank at the same time.\nShared memory is composed of 32 memory modules (banks), each 32 bits wide. If we place 64 fp32 values into SRAM, they are distributed as shown below: each value is assigned sequentially across the 32 banks.\nWhen each thread accesses a different bank (cases 1 and 2 in the diagram), there is no bank conflict. In case 3, bank conflicts may occurand we need to distinguish between two sub-cases:\nThreads access the same address in the same bank: for example, threads 0–7 all access element 0 in SRAM. This is treated as a broadcast access, so no bank conflict occurs. A single memory request satisfies all 32 threads. (You can think of a memory request as operating on 128-byte units.) Threads access different addresses in the same bank: for example, threads 0–3 access element 0 in SRAM and threads 4–7 access element 32. In this case, the first memory request cannot also fetch the data at address 32, because bank 0 is already busy returning address 0. An additional memory request is required, which serializes the accesses and hurts performance. This situation is what we call a bank conflict. Experiment To see how this behaves in practice, we can run the following four kernels. They operate on a 32×32 matrix; for simplicity the figures illustrate the 8×8 case. The data is stored in row-major order.\nKernel 0: ideal case Kernel 0 is the ideal case. The 32 threads in a warp sequentially load elements 0–31 from the input in DRAM and store them into addresses 0–31 in SRAM. Then they read those values back from SRAM and write them to addresses 0–31 in the output in DRAM.\nKernel 1: bank conflicts case Kernel 1 is written to intentionally create bank conflicts. It loads data from DRAM and forces all accesses to go through bank 0 in SRAM. As a result, there are 31 bank conflicts, which means the warp is effectively split into 32 serialized wavefronts.\nKernel 2: good case In kernel 2, each thread accesses a different bank, but the addresses in DRAM are not contiguous. This is still fine: each memory controller is responsible for its own region of memory and can service those requests independently, so there is no performance penalty from bank conflicts. However, the indexing pattern is somewhat artificial and not always practical in real kernels — hence it is a “good case” mainly from the bank-conflict perspective.\nKernel 3: DRAM load overhead case This case illustrates how important DRAM memory coalescing is. It is similar to kernel 2, but when loading from DRAM each thread fetches data from a different sector. In the coalesced case, 32 threads accessing contiguous fp32 values need only 4 sectors (4 bytes × 32 = 128 bytes). In this kernel, only 4 bytes per sector are actually useful. The hardware can still service four sectors per memory transaction, so we end up with 8 wavefronts, but overall we read 1024 bytes of DRAM to use only 128 bytes of data, which clearly degrades performance.\nConclusion As these four kernels show, an essential part of CUDA programming is arranging data so that it can be processed efficiently in bulk. By carefully choosing the data layout and access patterns, you can avoid bank conflicts and prevent unnecessary performance loss.\nHow about fp64? As an aside, how does this work for fp64 data, given that the SRAM bank size is 4 bytes? Assuming aligned and coalesced memory accesses, the memory controller is designed to split a 256-byte fp64 transaction into two 128-byte requests. In this setup, bank conflicts do not occur, even though each value is 64 bits.\nReferences https://forums.developer.nvidia.com/t/requesting-clarification-for-shared-memory-2024-12-02-bank-conflicts-and-shared-memory-access/268574/3 ","date":"2024-12-02T00:00:00Z","permalink":"https://jinseok-moon.github.io/p/bank-conflict/","title":"Shared memory: bank conflicts"}]