<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="This post is my study notes based on the excellent worklog. I rewrote the kernels and diagrams myself while following along.\nCUDA provides highly optimized GEMM APIs in cuBLAS, but with enough care we can write custom kernels that approach cuBLAS performance. Below, we will gradually apply core CUDA optimization ideas to a simple GEMM kernel.\nA: (M, K), row-major B: (K, N), row-major C: (M, N), row-major DRAM: Global memory SRAM: Shared memory We implement a series of kernels and compare their performance. The results are:\n"><title>GEMM 1</title><link rel=canonical href=https://jinseok-moon.github.io/p/gemm-1/><link rel=stylesheet href=/scss/style.min.44c91e7e98aee33a9cb385aa3c14716697fe72e9c673e06c0b7b2b5084608b31.css><meta property='og:title' content="GEMM 1"><meta property='og:description' content="This post is my study notes based on the excellent worklog. I rewrote the kernels and diagrams myself while following along.\nCUDA provides highly optimized GEMM APIs in cuBLAS, but with enough care we can write custom kernels that approach cuBLAS performance. Below, we will gradually apply core CUDA optimization ideas to a simple GEMM kernel.\nA: (M, K), row-major B: (K, N), row-major C: (M, N), row-major DRAM: Global memory SRAM: Shared memory We implement a series of kernels and compare their performance. The results are:\n"><meta property='og:url' content='https://jinseok-moon.github.io/p/gemm-1/'><meta property='og:site_name' content='Jinseok Moon'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='cuda'><meta property='article:tag' content='nvidia'><meta property='article:published_time' content='2025-10-26T00:00:00+00:00'><meta property='article:modified_time' content='2025-10-26T00:00:00+00:00'><meta name=twitter:title content="GEMM 1"><meta name=twitter:description content="This post is my study notes based on the excellent worklog. I rewrote the kernels and diagrams myself while following along.\nCUDA provides highly optimized GEMM APIs in cuBLAS, but with enough care we can write custom kernels that approach cuBLAS performance. Below, we will gradually apply core CUDA optimization ideas to a simple GEMM kernel.\nA: (M, K), row-major B: (K, N), row-major C: (M, N), row-major DRAM: Global memory SRAM: Shared memory We implement a series of kernels and compare their performance. The results are:\n"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><div class=site-meta><h1 class=site-name><a href=/>Jinseok Moon</a></h1><h2 class=site-description>GPU Software Engineer</h2></div></header><ol class=menu-social><li><a href=https://www.github.com/jinseok-moon target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/jinseok-moon target=_blank title=LinkedIn rel=me><svg class="icon icon-tabler icon-tabler-brand-linkedin" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="4" width="16" height="16" rx="2"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#0-naive-implementation>0. Naive implementation</a><ol><li><a href=#dram-coalescing>DRAM coalescing</a></li></ol></li><li><a href=#1-sram-caching>1. SRAM caching</a></li><li><a href=#2-sram-1d-tiling>2. SRAM 1D tiling</a></li><li><a href=#references>References</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/cuda/ style=background-color:#2a9d8f;color:#fff>cuda</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/gemm-1/>GEMM 1</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2025-10-26T00:00:00Z>Oct 26, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>6 minute read</time></div></footer><footer class=article-translations><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><div><a href=https://jinseok-moon.github.io/ko/p/gemm-1/ class=link>Korean</a></div></footer></div></header><section class=article-content><p>This post is my study notes based on the excellent <a class=link href=https://siboehm.com/articles/22/CUDA-MMM target=_blank rel=noopener>worklog</a>. I rewrote the kernels and diagrams myself while following along.</p><p>CUDA provides highly optimized GEMM APIs in cuBLAS, but with enough care we can write custom kernels that approach cuBLAS performance. Below, we will gradually apply core CUDA optimization ideas to a simple GEMM kernel.</p><ul><li>A: (M, K), row-major</li><li>B: (K, N), row-major</li><li>C: (M, N), row-major</li><li>DRAM: Global memory</li><li>SRAM: Shared memory</li></ul><p>We implement a series of kernels and compare their performance. The results are:</p><ol><li>Naive implementation, DRAM coalescing</li><li>SRAM caching</li><li>SRAM 1d tiling</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>[BENCHMARK]                    CUBLAS GEMM │ 0.045334 ms (w:10 r:20)
</span></span><span class=line><span class=cl>[BENCHMARK]               GPU GEMM 0 NAIVE │ 3.943722 ms (w:10 r:20) [PASSED]
</span></span><span class=line><span class=cl>[BENCHMARK]     GPU GEMM 0 DRAM COALESCING │ 0.517949 ms (w:10 r:20) [PASSED]
</span></span><span class=line><span class=cl>[BENCHMARK]        GPU GEMM 1 SRAM CACHING │ 0.248670 ms (w:10 r:20) [PASSED]
</span></span><span class=line><span class=cl>[BENCHMARK]      GPU GEMM 2 SRAM 1D TILING │ 0.249046 ms (w:10 r:20) [PASSED]
</span></span></code></pre></div><h2 id=0-naive-implementation>0. Naive implementation</h2><p><img src=/p/gemm-1/images/image.png width=1207 height=375 loading=lazy class=gallery-image data-flex-grow=321 data-flex-basis=772px></p><p>The most basic implementation assigns one element of $C$ to each thread. Because the matrices are stored in row-major order, this layout forces us to read non-contiguous memory for $B$. The following figure shows how this looks at the warp level.</p><p><img src=/p/gemm-1/images/image-1.png width=1224 height=413 loading=lazy class=gallery-image data-flex-grow=296 data-flex-basis=711px></p><p>In this loop structure, when loading $A$ each thread in a warp accesses a different column, so the accesses are not contiguous and cannot be coalesced. Without coalescing, the hardware effectively has to service 32 separate loads per warp, which significantly degrades performance.</p><p>When loading $B$, all threads in the warp access the same element, so the hardware can use a broadcast. Even so, given the overall access pattern, there is little benefit from grouping these threads into a single warp in this naïve mapping.</p><h3 id=dram-coalescing>DRAM coalescing</h3><p><img src=/p/gemm-1/images/image-2.png width=1230 height=404 loading=lazy class=gallery-image data-flex-grow=304 data-flex-basis=730px></p><p>To leverage the warp properly, we need to access contiguous memory as shown above. When loading $A$, we can exploit broadcast to share values within the warp. When loading $B$, each thread in the warp accesses adjacent elements, so the hardware can coalesce the loads into 128‑byte transactions.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>gemm_gpu_0_naive</span><span class=p>(</span><span class=kt>int</span> <span class=n>M</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>,</span> <span class=kt>int</span> <span class=n>K</span><span class=p>,</span> <span class=kt>float</span> <span class=n>alpha</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>A</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> <span class=kt>float</span> <span class=n>beta</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>C</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>tid</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>row</span> <span class=o>=</span> <span class=n>tid</span> <span class=o>%</span> <span class=n>N</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>col</span> <span class=o>=</span> <span class=n>tid</span> <span class=o>/</span> <span class=n>N</span><span class=p>;</span>
</span></span><span class=line><span class=cl> <span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>gemm_gpu_0_dram_coalescing</span><span class=p>(</span><span class=kt>int</span> <span class=n>M</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>,</span> <span class=kt>int</span> <span class=n>K</span><span class=p>,</span> <span class=kt>float</span> <span class=n>alpha</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>A</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> <span class=kt>float</span> <span class=n>beta</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>C</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>tid</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>row</span> <span class=o>=</span> <span class=n>tid</span> <span class=o>/</span> <span class=n>N</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>col</span> <span class=o>=</span> <span class=n>tid</span> <span class=o>%</span> <span class=n>N</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The only difference between the two kernels is how <code>row</code> and <code>col</code> are computed, yet the performance difference is large. Profiling shows that this gap comes from DRAM operations.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># RTX 5090</span>
</span></span><span class=line><span class=cl>$ sudo /usr/local/cuda/bin/ncu --metrics dram__bytes.sum.per_second gemm
</span></span><span class=line><span class=cl>  gemm_gpu_0_naive<span class=o>(</span>int, int, int, float, float *, float *, float, float *<span class=o>)</span> <span class=o>(</span>4096, 1, 1<span class=o>)</span>x<span class=o>(</span>256, 1, 1<span class=o>)</span>, Context 1, Stream 7, Device 0, CC 12.0
</span></span><span class=line><span class=cl>    Section: Command line profiler metrics
</span></span><span class=line><span class=cl>    -------------------------- ----------- ------------
</span></span><span class=line><span class=cl>    Metric Name                Metric Unit Metric Value
</span></span><span class=line><span class=cl>    -------------------------- ----------- ------------
</span></span><span class=line><span class=cl>    dram__bytes.sum.per_second     Gbyte/s         2.40
</span></span><span class=line><span class=cl>    -------------------------- ----------- ------------
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  gemm_gpu_0_dram_coalescing<span class=o>(</span>int, int, int, float, float *, float *, float, float *<span class=o>)</span> <span class=o>(</span>4096, 1, 1<span class=o>)</span>x<span class=o>(</span>256, 1, 1<span class=o>)</span>, Context 1, Stream 7, Device 0, CC 12.0
</span></span><span class=line><span class=cl>    Section: Command line profiler metrics
</span></span><span class=line><span class=cl>    -------------------------- ----------- ------------
</span></span><span class=line><span class=cl>    Metric Name                Metric Unit Metric Value
</span></span><span class=line><span class=cl>    -------------------------- ----------- ------------
</span></span><span class=line><span class=cl>    dram__bytes.sum.per_second     Gbyte/s        18.07
</span></span><span class=line><span class=cl>    -------------------------- ----------- ------------
</span></span></code></pre></div><h2 id=1-sram-caching>1. SRAM caching</h2><p>In the naïve kernel, the same data is fetched from DRAM many times, which is very expensive. According to this <a class=link href=https://arxiv.org/abs/1804.06826 target=_blank rel=noopener>paper</a>, on a V100 the DRAM bandwidth is about 900 GB/s, while the shared-memory (SRAM) bandwidth is around 13,800 GB/s (the exact SRAM number is not officially documented). We therefore want to cache data in shared memory and reuse it as much as possible.</p><p><img src=/p/gemm-1/images/image-3.png width=1072 height=1040 loading=lazy class=gallery-image data-flex-grow=103 data-flex-basis=247px></p><p>Each block is responsible for a 32×32 tile of $C$. The shaded regions in the figure indicate the corresponding tiles of $A$ and $B$ that must be loaded from DRAM. The <code>bkIdx</code> loop walks along $K$ in chunks of <code>BLOCKSIZE</code>, loading tiles into shared memory; the <code>tIter</code> loop then performs the GEMM on those tiles. Since each thread produces a single output element, it accumulates the partial result into a scalar <code>sum</code>, which is finally written back to the appropriate location in $C$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>template</span> <span class=o>&lt;</span><span class=kt>int</span> <span class=n>BLOCKSIZE</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=n>gemm_gpu_1_sram_caching</span><span class=p>(</span><span class=kt>int</span> <span class=n>M</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>,</span> <span class=kt>int</span> <span class=n>K</span><span class=p>,</span> <span class=kt>float</span> <span class=n>alpha</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>A</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> <span class=kt>float</span> <span class=n>beta</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>C</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>bkRow</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>bkCol</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>A</span> <span class=o>+=</span> <span class=n>K</span> <span class=o>*</span> <span class=n>BLOCKSIZE</span> <span class=o>*</span> <span class=n>bkRow</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>B</span> <span class=o>+=</span> <span class=n>BLOCKSIZE</span> <span class=o>*</span> <span class=n>bkCol</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>C</span> <span class=o>+=</span> <span class=n>N</span> <span class=o>*</span> <span class=n>BLOCKSIZE</span> <span class=o>*</span> <span class=n>bkRow</span> <span class=o>+</span> <span class=n>BLOCKSIZE</span> <span class=o>*</span> <span class=n>bkCol</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>sA</span><span class=p>[</span><span class=n>BLOCKSIZE</span> <span class=o>*</span> <span class=n>BLOCKSIZE</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>sB</span><span class=p>[</span><span class=n>BLOCKSIZE</span> <span class=o>*</span> <span class=n>BLOCKSIZE</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>tRow</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>/</span> <span class=n>BLOCKSIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>tCol</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>%</span> <span class=n>BLOCKSIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>sum</span> <span class=o>=</span> <span class=mf>0.0f</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>bkIdx</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>bkIdx</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>bkIdx</span> <span class=o>+=</span> <span class=n>BLOCKSIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>sA</span><span class=p>[</span><span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>tRow</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=n>tCol</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=n>sB</span><span class=p>[</span><span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[</span><span class=n>tRow</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>tCol</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>A</span> <span class=o>+=</span> <span class=n>BLOCKSIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span> <span class=o>+=</span> <span class=n>BLOCKSIZE</span> <span class=o>*</span> <span class=n>N</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>tIter</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>tIter</span> <span class=o>&lt;</span> <span class=n>BLOCKSIZE</span><span class=p>;</span> <span class=n>tIter</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>sum</span> <span class=o>+=</span> <span class=n>sA</span><span class=p>[</span><span class=n>tRow</span> <span class=o>*</span> <span class=n>BLOCKSIZE</span> <span class=o>+</span> <span class=n>tIter</span><span class=p>]</span> <span class=o>*</span> <span class=n>sB</span><span class=p>[</span><span class=n>tIter</span> <span class=o>*</span> <span class=n>BLOCKSIZE</span> <span class=o>+</span> <span class=n>tCol</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>C</span><span class=p>[</span><span class=n>tRow</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>tCol</span><span class=p>]</span> <span class=o>=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>sum</span> <span class=o>+</span> <span class=n>beta</span> <span class=o>*</span> <span class=n>C</span><span class=p>[</span><span class=n>tRow</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>tCol</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=2-sram-1d-tiling>2. SRAM 1D tiling</h2><p>SRAM caching significantly improves performance, but it is still not on par with cuBLAS. In the current design, each thread produces one output element. Its memory access pattern looks like this:
for each element of $C$, we need approximately <code>K/16</code> DRAM loads and <code>K*2</code> shared-memory loads.</p><ul><li>DRAM: K/32 iterations of outer loop * 2 loads</li><li>SRAM: K/32 iterations of outer loop * BLOCKSIZE (=32) * 2 loads</li><li>Memory accesses per result: K / 16 DRAM, K * 2 SRAM</li></ul><p>Profiling shows that warps often stall on memory input/output (MIO), confirming that memory traffic is the main bottleneck.</p><p><img src=/p/gemm-1/images/image-4.png width=2072 height=602 loading=lazy class=gallery-image data-flex-grow=344 data-flex-basis=826px></p><p>We can alleviate this by reusing each loaded value more aggressively. If each thread computes 8 output elements instead of just 1 (a 1D tiling in the $M$ dimension), the access pattern becomes:</p><p><img src=/p/gemm-1/images/image-5.png width=1880 height=1042 loading=lazy class=gallery-image data-flex-grow=180 data-flex-basis=433px></p><p>Within a warp, each thread now computes 8 elements of $C$ along the column direction. Recomputing the memory accesses per result under this scheme gives us:</p><ul><li>DRAM: K/8 iters (dotIdx) loop * 2 loads</li><li>SRAM: K/8 iters (dotIdx) loop * BK(=8) * (1 + TM(=8))</li><li>Memory accesses per result: K/32 DRAM, K * 9/8 SRAM</li></ul><p>This reduces the number of memory accesses per output element from <code>K/16</code> to <code>K/32</code> for DRAM and from <code>K*2</code> to <code>K*9/8</code> for shared memory.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>bkIdx</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>bkIdx</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>bkIdx</span> <span class=o>+=</span> <span class=n>BK</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>sA</span><span class=p>[</span><span class=n>innerRowA</span> <span class=o>*</span> <span class=n>BK</span> <span class=o>+</span> <span class=n>innerColA</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>innerRowA</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=n>innerColA</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>sB</span><span class=p>[</span><span class=n>innerRowB</span> <span class=o>*</span> <span class=n>BN</span> <span class=o>+</span> <span class=n>innerColB</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[</span><span class=n>innerRowB</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>innerRowB</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>A</span> <span class=o>+=</span> <span class=n>BK</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>B</span> <span class=o>+=</span> <span class=n>BK</span> <span class=o>*</span> <span class=n>N</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>dotIdx</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>dotIdx</span> <span class=o>&lt;</span> <span class=n>BK</span><span class=p>;</span> <span class=n>dotIdx</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>_b</span> <span class=o>=</span> <span class=n>sB</span><span class=p>[</span><span class=n>dotIdx</span> <span class=o>*</span> <span class=n>BN</span> <span class=o>+</span> <span class=n>tCol</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>resIdx</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>resIdx</span> <span class=o>&lt;</span> <span class=n>TM</span><span class=p>;</span> <span class=n>resIdx</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>sum</span><span class=p>[</span><span class=n>resIdx</span><span class=p>]</span> <span class=o>+=</span> <span class=n>sA</span><span class=p>[(</span><span class=n>tRow</span> <span class=o>*</span> <span class=n>TM</span> <span class=o>+</span> <span class=n>resIdx</span><span class=p>)</span> <span class=o>*</span> <span class=n>BK</span> <span class=o>+</span> <span class=n>dotIdx</span><span class=p>]</span> <span class=o>*</span> <span class=n>_b</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>In this kernel, <code>BM</code> and <code>TM</code> must be chosen so that <code>BM == TM * (number of results per thread in M)</code>. The number of threads per block is <code>(BM * BN / TM)</code>and we rely on that to match the sizes of <code>sA</code> and <code>sB</code> so that the DRAM→SRAM loads can be implemented with simple strided accesses.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>sA</span><span class=p>[</span><span class=n>innerRowA</span> <span class=o>*</span> <span class=n>BK</span> <span class=o>+</span> <span class=n>innerColA</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>innerRowA</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=n>innerColA</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>sB</span><span class=p>[</span><span class=n>innerRowB</span> <span class=o>*</span> <span class=n>BN</span> <span class=o>+</span> <span class=n>innerColB</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[</span><span class=n>innerRowB</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>innerRowB</span><span class=p>];</span>
</span></span></code></pre></div><h2 id=references>References</h2><ul><li><a class=link href=https://siboehm.com/articles/22/CUDA-MMM target=_blank rel=noopener>How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/cuda/>Cuda</a>
<a href=/tags/nvidia/>Nvidia</a></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=[".main-article",".widget--toc"];e.forEach(e=>{const t=document.querySelector(e);t&&renderMathInElement(t,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/archs/><div class=article-details><h2 class=article-title>GPU architecture w/ LLM</h2></div></a></article><article><a href=/p/pinned-memory/><div class=article-details><h2 class=article-title>Pageable vs. pinned data transfer</h2></div></a></article><article><a href=/p/cuda-tile/><div class=article-details><h2 class=article-title>CUDA Tile IR, cuTile Python</h2></div></a></article><article><a href=/p/gemm-2/><div class=article-details><h2 class=article-title>GEMM 2</h2></div></a></article><article><a href=/p/thread-indexing/><div class=article-details><h2 class=article-title>Proper thread indexing and memory coalescing</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//myblog-otcr7pydaj.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2024 -
2026 Jinseok Moon</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.33.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>