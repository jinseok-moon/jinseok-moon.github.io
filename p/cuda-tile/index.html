<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Until now Traditional CUDA programming has been based on CUDA C++. It requires mapping data to blocks and threads based on SIMT. While careful design can achieve optimal performance, it&rsquo;s not something everyone can easily do. Additionally, as GPU architectures evolve, hardware specifications change, so kernel developers had to develop different optimal kernels for each GPU.\nIn response to these challenges, triton-language emerged (not related to NVIDIA Triton Inference Server). Triton improved accessibility through block-level abstractions such as memory management, synchronization, and tensor core scheduling. Furthermore, through MLIR, it has expanded as a means to connect to various hardware backends including not only NVIDIA GPUs but also AMD and NPUs.\n"><title>CUDA Tile IR, cuTile Python</title><link rel=canonical href=https://jinseok-moon.github.io/p/cuda-tile/><link rel=stylesheet href=/scss/style.min.44c91e7e98aee33a9cb385aa3c14716697fe72e9c673e06c0b7b2b5084608b31.css><meta property='og:title' content="CUDA Tile IR, cuTile Python"><meta property='og:description' content="Until now Traditional CUDA programming has been based on CUDA C++. It requires mapping data to blocks and threads based on SIMT. While careful design can achieve optimal performance, it&rsquo;s not something everyone can easily do. Additionally, as GPU architectures evolve, hardware specifications change, so kernel developers had to develop different optimal kernels for each GPU.\nIn response to these challenges, triton-language emerged (not related to NVIDIA Triton Inference Server). Triton improved accessibility through block-level abstractions such as memory management, synchronization, and tensor core scheduling. Furthermore, through MLIR, it has expanded as a means to connect to various hardware backends including not only NVIDIA GPUs but also AMD and NPUs.\n"><meta property='og:url' content='https://jinseok-moon.github.io/p/cuda-tile/'><meta property='og:site_name' content='Jinseok Moon'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2026-01-12T00:00:00+00:00'><meta property='article:modified_time' content='2026-01-12T00:00:00+00:00'><meta name=twitter:title content="CUDA Tile IR, cuTile Python"><meta name=twitter:description content="Until now Traditional CUDA programming has been based on CUDA C++. It requires mapping data to blocks and threads based on SIMT. While careful design can achieve optimal performance, it&rsquo;s not something everyone can easily do. Additionally, as GPU architectures evolve, hardware specifications change, so kernel developers had to develop different optimal kernels for each GPU.\nIn response to these challenges, triton-language emerged (not related to NVIDIA Triton Inference Server). Triton improved accessibility through block-level abstractions such as memory management, synchronization, and tensor core scheduling. Furthermore, through MLIR, it has expanded as a means to connect to various hardware backends including not only NVIDIA GPUs but also AMD and NPUs.\n"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><div class=site-meta><h1 class=site-name><a href=/>Jinseok Moon</a></h1><h2 class=site-description>GPU Software Engineer</h2></div></header><ol class=menu-social><li><a href=https://www.github.com/jinseok-moon target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/jinseok-moon target=_blank title=LinkedIn rel=me><svg class="icon icon-tabler icon-tabler-brand-linkedin" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="4" width="16" height="16" rx="2"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#until-now>Until now</a></li><li><a href=#cuda-tile-ir>CUDA Tile IR</a></li><li><a href=#cutile-python-how-to-install>cuTile Python: how-to install</a></li><li><a href=#key-concepts-tile-vs-simt>Key concepts: Tile vs. SIMT</a><ol><li><a href=#kernel-definition>Kernel Definition</a></li></ol></li><li><a href=#tutorial-w-vector-addition>Tutorial w/ vector addition</a><ol><li><a href=#1d-tile>1D Tile</a></li><li><a href=#2d-tile>2D Tile</a></li><li><a href=#when-tiles-dont-fit>When tiles don&rsquo;t fit</a></li></ol></li><li><a href=#performance>Performance</a></li><li><a href=#references>References</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/cuda/ style=background-color:#2a9d8f;color:#fff>cuda</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/cuda-tile/>CUDA Tile IR, cuTile Python</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2026-01-12T00:00:00Z>Jan 12, 2026</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>6 minute read</time></div></footer><footer class=article-translations><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><div><a href=https://jinseok-moon.github.io/ko/p/cuda-tile/ class=link>Korean</a></div></footer></div></header><section class=article-content><h2 id=until-now>Until now</h2><p>Traditional CUDA programming has been based on CUDA C++. It requires mapping data to blocks and threads based on SIMT. While careful design can achieve optimal performance, it&rsquo;s not something everyone can easily do. Additionally, as GPU architectures evolve, hardware specifications change, so kernel developers had to develop different optimal kernels for each GPU.</p><p>In response to these challenges, triton-language emerged (<a class=link href=https://github.com/triton-lang/triton/issues/156 target=_blank rel=noopener>not related to NVIDIA Triton Inference Server</a>). Triton improved accessibility through block-level abstractions such as memory management, synchronization, and tensor core scheduling. Furthermore, through MLIR, it has expanded as a means to connect to various hardware backends including not only NVIDIA GPUs but also AMD and NPUs.</p><p>Of course, NVIDIA also provides libraries like CUTLASS to overcome these difficulties. By templating small tile-level operations, developers can relatively easily develop optimal kernels. FlashAttention-3 is also written based on CUTLASS. However, &ldquo;relatively easy&rdquo; is not truly easy. You still need to know numerous templates and logic. This also ultimately requires knowledge of CUDA C++.</p><h2 id=cuda-tile-ir>CUDA Tile IR</h2><p>Then came the announcement in spring 2025. News that NVIDIA is developing a tile-based programming model. I had been waiting for it to be released, and they released it before 2025 ended.</p><p><img src=/p/cuda-tile/images/image-1.png width=1190 height=1212 loading=lazy class=gallery-image data-flex-grow=98 data-flex-basis=235px></p><p>cuTile remains at the same level as the existing SIMT-based programming model. This means that in addition to the existing path of CUDA C++ code development -> PTX -> CUBIN, a new path of Tile IR -> CUBIN has been added.</p><p><img src=/p/cuda-tile/images/image.png width=571 height=970 loading=lazy class=gallery-image data-flex-grow=58 data-flex-basis=141px></p><p>Tile IR is planned to be based on MLIR Dialect and will be stored as bytecode. Unless you&rsquo;re developing parts that directly interact with Tile IR, you can use cuTile Python.</p><ul><li><strong>NVIDIA cuTile Python</strong>: Most developers fall into this category. It&rsquo;s a Python implementation that uses CUDA Tile IR as its backend.</li><li><strong>CUDA Tile IR</strong>: Developers who want to build their own DSL compilers or libraries use CUDA Tile IR.</li></ul><h2 id=cutile-python-how-to-install>cuTile Python: how-to install</h2><p>The development environment is from the official documentation.</p><ul><li>Linux x86_64, Linux aarch64 or Windows x86_64</li><li>A GPU with compute capability 10.x or 12.x</li><li>NVIDIA Driver r580 or later</li><li>CUDA Toolkit 13.1 or later</li><li>Python version 3.10, 3.11, 3.12 or 3.13</li></ul><p>Once the environment is set up, install via pip.
Install torch according to your environment as well.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install cuda-tile
</span></span><span class=line><span class=cl>pip install cupy-cuda13x  <span class=c1># For sample</span>
</span></span><span class=line><span class=cl>pip install pytest numpy  <span class=c1># For test</span>
</span></span></code></pre></div><h2 id=key-concepts-tile-vs-simt>Key concepts: Tile vs. SIMT</h2><p><img src=/p/cuda-tile/images/image-2.png width=738 height=603 loading=lazy alt="Tile-based programming" class=gallery-image data-flex-grow=122 data-flex-basis=293px></p><p>The tile model (left) partitions data into blocks and the compiler maps them to threads. The SIMT model (right) maps data to both blocks and threads. SIMT allows control of each individual thread, but achieving optimal performance requires manual tuning that considers hardware complexity. The tile model abstracts some of the hardware complexity, allowing the CUDA compiler/runtime to handle tile algorithms internally, while users can focus on algorithm development.</p><h3 id=kernel-definition>Kernel Definition</h3><p>The <code>@ct.kernel</code> decorator compiles a Python function into a GPU kernel. As we learned above, the content of this kernel doesn&rsquo;t generate PTX through CUDA C++, but rather goes down to <code>Tile IR</code> and generates cubin through MLIR using <a class=link href=https://github.com/NVIDIA/cuda-tile target=_blank rel=noopener>CUDA TILE IR</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>cuda.tile</span> <span class=k>as</span> <span class=nn>ct</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@ct.kernel</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>vector_add</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>tile_size</span><span class=p>:</span> <span class=n>ct</span><span class=o>.</span><span class=n>Constant</span><span class=p>[</span><span class=nb>int</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>kernel</span><span class=p>(</span><span class=n>TileDispatcher</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>constant_flags</span> <span class=o>=</span> <span class=n>get_constant_arg_flags</span><span class=p>(</span><span class=n>function</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>compiler_options</span> <span class=o>=</span> <span class=n>CompilerOptions</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>num_ctas</span><span class=o>=</span><span class=n>num_ctas</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>occupancy</span><span class=o>=</span><span class=n>occupancy</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>opt_level</span><span class=o>=</span><span class=n>opt_level</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>compile</span> <span class=o>=</span> <span class=n>_compile</span><span class=o>.</span><span class=n>CompileCallback</span><span class=p>(</span><span class=n>function</span><span class=p>,</span> <span class=n>compiler_options</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>constant_flags</span><span class=p>,</span> <span class=nb>compile</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>_pyfunc</span> <span class=o>=</span> <span class=n>function</span>
</span></span></code></pre></div><h2 id=tutorial-w-vector-addition>Tutorial w/ vector addition</h2><p>Let&rsquo;s learn how to use it through vector addition kernel, the most basic CUDA programming example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>vecAdd</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>A</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>B</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>C</span><span class=p>,</span> <span class=kt>int</span> <span class=n>vectorLength</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl> <span class=cm>/* calculate my thread index */</span>
</span></span><span class=line><span class=cl> <span class=kt>int</span> <span class=n>workIndex</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=o>*</span><span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> <span class=k>if</span><span class=p>(</span><span class=n>workIndex</span> <span class=o>&lt;</span> <span class=n>vectorLength</span><span class=p>)</span>
</span></span><span class=line><span class=cl> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=cm>/* perform the vector addition */</span>
</span></span><span class=line><span class=cl>  <span class=n>C</span><span class=p>[</span><span class=n>workIndex</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>workIndex</span><span class=p>]</span> <span class=o>+</span> <span class=n>B</span><span class=p>[</span><span class=n>workIndex</span><span class=p>];</span>
</span></span><span class=line><span class=cl> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h3 id=1d-tile>1D Tile</h3><p>This is the simplest form of a cuTile kernel.</p><ul><li>Load one or more tiles from GPU memory</li><li>Perform computation on tiles to produce new tiles</li><li>Store result tiles to GPU memory</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>cuda.tile</span> <span class=k>as</span> <span class=nn>ct</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@ct.kernel</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>vector_add</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>tile_size</span><span class=p>:</span> <span class=n>ct</span><span class=o>.</span><span class=n>Constant</span><span class=p>[</span><span class=nb>int</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>  <span class=c1># Get the 1D pid, blockIdx.x in cuda c++</span>
</span></span><span class=line><span class=cl>  <span class=n>pid</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>bid</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Load input tiles</span>
</span></span><span class=line><span class=cl>  <span class=n>a_tile</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=p>(</span><span class=n>pid</span><span class=p>,)</span> <span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>tile_size</span><span class=p>,</span> <span class=p>)</span> <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>b_tile</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>b</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=p>(</span><span class=n>pid</span><span class=p>,)</span> <span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>tile_size</span><span class=p>,</span> <span class=p>)</span> <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Perform elementwise addition</span>
</span></span><span class=line><span class=cl>  <span class=n>result</span> <span class=o>=</span> <span class=n>a_tile</span> <span class=o>+</span> <span class=n>b_tile</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Store result</span>
</span></span><span class=line><span class=cl>  <span class=n>ct</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>c</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=p>(</span><span class=n>pid</span><span class=p>,</span> <span class=p>),</span> <span class=n>tile</span><span class=o>=</span><span class=n>result</span><span class=p>)</span>
</span></span></code></pre></div><p><code>ct.bid(0)</code> gets the block ID along axis-0. It&rsquo;s equivalent to getting <code>blockIdx</code> in CUDA C++. <code>ct.load()</code> loads data from memory at the required index and tile shape. Adding the loaded <code>a_tile</code> and <code>b_tile</code> is done with the <code>+</code> operator, and the result is held in the <code>result</code> tile, which hasn&rsquo;t been stored to DRAM yet, so we store it using <code>ct.store()</code>.</p><h3 id=2d-tile>2D Tile</h3><p>Reshaping a 1D tile to 2D makes it suitable for matrix operations. Indexing is truly intuitive as follows.</p><p><img src=/p/cuda-tile/images/image-3.png width=1152 height=648 loading=lazy alt="2D element and tile space" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@ct.kernel</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>vec_add_kernel_2d</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>TILE_X</span><span class=p>:</span> <span class=n>ct</span><span class=o>.</span><span class=n>Constant</span><span class=p>[</span><span class=nb>int</span><span class=p>],</span> <span class=n>TILE_Y</span><span class=p>:</span> <span class=n>ct</span><span class=o>.</span><span class=n>Constant</span><span class=p>[</span><span class=nb>int</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Get the global IDs of the current block along the X and Y axes.</span>
</span></span><span class=line><span class=cl>    <span class=c1># `ct.bid(0)` for the first grid dimension (typically rows),</span>
</span></span><span class=line><span class=cl>    <span class=c1># `ct.bid(1)` for the second grid dimension (typically columns).</span>
</span></span><span class=line><span class=cl>    <span class=n>bid_x</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>bid</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>bid_y</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>bid</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Load `TILE_X` x `TILE_Y` chunks from input matrices &#39;a&#39; and &#39;b&#39;.</span>
</span></span><span class=line><span class=cl>    <span class=c1># The `index=(bid_x, bid_y)` specifies the 2D tile to load.</span>
</span></span><span class=line><span class=cl>    <span class=n>a_tile</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=p>(</span><span class=n>bid_x</span><span class=p>,</span> <span class=n>bid_y</span><span class=p>),</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>TILE_X</span><span class=p>,</span> <span class=n>TILE_Y</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>b_tile</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>b</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=p>(</span><span class=n>bid_x</span><span class=p>,</span> <span class=n>bid_y</span><span class=p>),</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>TILE_X</span><span class=p>,</span> <span class=n>TILE_Y</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Perform the element-wise addition on the loaded tiles.</span>
</span></span><span class=line><span class=cl>    <span class=n>sum_tile</span> <span class=o>=</span> <span class=n>a_tile</span> <span class=o>+</span> <span class=n>b_tile</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Store the resulting `TILE_X` x `TILE_Y` chunk back to the output matrix &#39;c&#39;.</span>
</span></span><span class=line><span class=cl>    <span class=n>ct</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>c</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=p>(</span><span class=n>bid_x</span><span class=p>,</span> <span class=n>bid_y</span><span class=p>),</span> <span class=n>tile</span><span class=o>=</span><span class=n>sum_tile</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=when-tiles-dont-fit>When tiles don&rsquo;t fit</h3><p>The two examples above are cases where the total data size divides evenly by the tile size. For cases that don&rsquo;t align with tiles, use <code>ct.gather()</code> and <code>ct.scatter()</code>. This is also recommended for non-power-of-two cases. When out of bounds, <code>padding_value (default: 0)</code> is returned.</p><p><img src=/p/cuda-tile/images/image-4.png width=1320 height=840 loading=lazy class=gallery-image data-flex-grow=157 data-flex-basis=377px></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@ct.kernel</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>vec_add_kernel_2d_gather</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>TILE_X</span><span class=p>:</span> <span class=n>ConstInt</span><span class=p>,</span> <span class=n>TILE_Y</span><span class=p>:</span> <span class=n>ConstInt</span>  <span class=c1># Tile dimensions for this block</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Get the global IDs of the current block along the X and Y axes.</span>
</span></span><span class=line><span class=cl>    <span class=n>bid_x</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>bid</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>bid_y</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>bid</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Calculate X and Y indices within the current block&#39;s tile.</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>bid_x</span> <span class=o>*</span> <span class=n>TILE_X</span> <span class=o>+</span> <span class=n>ct</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>TILE_X</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>bid_y</span> <span class=o>*</span> <span class=n>TILE_Y</span> <span class=o>+</span> <span class=n>ct</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>TILE_Y</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Reshape the X and Y indices to (TILE_X, 1) and (1, TILE_Y), respectively.</span>
</span></span><span class=line><span class=cl>    <span class=c1># This way, they can be broadcasted together to a common shape (TILE_X, TILE_Y).</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=p>[:,</span> <span class=kc>None</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Load elements using the calculated X and Y indices.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Both `a_tile` and `b_tile` have shape (TILE_X, TILE_Y).</span>
</span></span><span class=line><span class=cl>    <span class=n>a_tile</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>b_tile</span> <span class=o>=</span> <span class=n>ct</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=n>b</span><span class=p>,</span> <span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Perform the element-wise addition.</span>
</span></span><span class=line><span class=cl>    <span class=n>sum_tile</span> <span class=o>=</span> <span class=n>a_tile</span> <span class=o>+</span> <span class=n>b_tile</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Store the result back to `c` using the same index tiles.</span>
</span></span><span class=line><span class=cl>    <span class=c1># `ct.scatter()` only writes data to positions within the array bounds.</span>
</span></span><span class=line><span class=cl>    <span class=n>ct</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>c</span><span class=p>,</span> <span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>),</span> <span class=n>sum_tile</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=performance>Performance</h2><p>Running <code>cutile-python/test/bench_matmul.py</code> gives the following results. The Torch kernel internally calls the cutlass kernel. While performance is still slower compared to torch, it&rsquo;s promising that tile-based programming has become possible, significantly reducing kernel implementation difficulty. Since the IR has also been released as open source, performance will gradually improve going forward.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>--------------------------------------------------------------------------------------------------- benchmark <span class=s1>&#39;matmul&#39;</span>: <span class=m>12</span> tests ---------------------------------------------------------------------------------------------------
</span></span><span class=line><span class=cl>Name <span class=o>(</span><span class=nb>time</span> in us<span class=o>)</span>                                 Min                     Max                    Mean              StdDev                  Median                 IQR            Outliers          OPS            Rounds  Iterations
</span></span><span class=line><span class=cl>------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
</span></span><span class=line><span class=cl>bench_matmul<span class=o>[</span>1K-1K-1K-f16-torch<span class=o>]</span>              14.3439 <span class=o>(</span>1.0<span class=o>)</span>           14.3451 <span class=o>(</span>1.0<span class=o>)</span>           14.3443 <span class=o>(</span>1.0<span class=o>)</span>        0.0005 <span class=o>(</span>1.0<span class=o>)</span>           14.3441 <span class=o>(</span>1.0<span class=o>)</span>        0.0005 <span class=o>(</span>1.0<span class=o>)</span>           1<span class=p>;</span><span class=m>0</span>  69,714.0067 <span class=o>(</span>1.0<span class=o>)</span>           <span class=m>5</span>        <span class=m>5244</span>
</span></span><span class=line><span class=cl>bench_matmul<span class=o>[</span>1K-1K-1K-f32-torch<span class=o>]</span>              26.6370 <span class=o>(</span>1.86<span class=o>)</span>          26.6386 <span class=o>(</span>1.86<span class=o>)</span>          26.6374 <span class=o>(</span>1.86<span class=o>)</span>       0.0006 <span class=o>(</span>1.37<span class=o>)</span>          26.6372 <span class=o>(</span>1.86<span class=o>)</span>       0.0005 <span class=o>(</span>1.02<span class=o>)</span>          1<span class=p>;</span><span class=m>1</span>  37,541.1889 <span class=o>(</span>0.54<span class=o>)</span>          <span class=m>5</span>        <span class=m>3218</span>
</span></span><span class=line><span class=cl>bench_matmul<span class=o>[</span>1K-1K-1K-f16-cutile<span class=o>]</span>            154.3699 <span class=o>(</span>10.76<span class=o>)</span>        154.4450 <span class=o>(</span>10.77<span class=o>)</span>        154.4040 <span class=o>(</span>10.76<span class=o>)</span>      0.0277 <span class=o>(</span>58.67<span class=o>)</span>        154.4062 <span class=o>(</span>10.76<span class=o>)</span>      0.0332 <span class=o>(</span>73.27<span class=o>)</span>         2<span class=p>;</span><span class=m>0</span>   6,476.5145 <span class=o>(</span>0.09<span class=o>)</span>          <span class=m>5</span>         <span class=m>613</span>
</span></span><span class=line><span class=cl>bench_matmul<span class=o>[</span>1K-1K-1K-f32-cutile<span class=o>]</span>            892.8400 <span class=o>(</span>62.25<span class=o>)</span>        893.2511 <span class=o>(</span>62.27<span class=o>)</span>        893.0082 <span class=o>(</span>62.26<span class=o>)</span>      0.1616 <span class=o>(</span>342.04<span class=o>)</span>       893.0112 <span class=o>(</span>62.26<span class=o>)</span>      0.2286 <span class=o>(</span>505.17<span class=o>)</span>        2<span class=p>;</span><span class=m>0</span>   1,119.8106 <span class=o>(</span>0.02<span class=o>)</span>          <span class=m>5</span>         <span class=m>112</span>
</span></span></code></pre></div><h2 id=references>References</h2><ul><li><a class=link href=https://docs.nvidia.com/cuda/cutile-python/ target=_blank rel=noopener>https://docs.nvidia.com/cuda/cutile-python/</a></li><li><a class=link href=https://developer.nvidia.com/blog/focus-on-your-algorithm-nvidia-cuda-tile-handles-the-hardware target=_blank rel=noopener>https://developer.nvidia.com/blog/focus-on-your-algorithm-nvidia-cuda-tile-handles-the-hardware</a></li><li><a class=link href=https://developer.nvidia.com/blog/simplify-gpu-programming-with-nvidia-cuda-tile-in-python target=_blank rel=noopener>https://developer.nvidia.com/blog/simplify-gpu-programming-with-nvidia-cuda-tile-in-python</a></li></ul></section><footer class=article-footer></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=[".main-article",".widget--toc"];e.forEach(e=>{const t=document.querySelector(e);t&&renderMathInElement(t,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/archs/><div class=article-details><h2 class=article-title>GPU architecture w/ LLM</h2></div></a></article><article><a href=/p/gemm-2/><div class=article-details><h2 class=article-title>GEMM 2</h2></div></a></article><article><a href=/p/gemm-1/><div class=article-details><h2 class=article-title>GEMM 1</h2></div></a></article><article><a href=/p/thread-indexing/><div class=article-details><h2 class=article-title>Proper thread indexing and memory coalescing</h2></div></a></article><article><a href=/p/online-softmax/><div class=article-details><h2 class=article-title>Online softmax</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//myblog-otcr7pydaj.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2024 -
2026 Jinseok Moon</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.33.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>