---
title: "CNN 02 - 순전파 / 역전파"
excerpt: "CNN 02 - Forward/Backward Propagation"
tags: [Python, Convolutional Neural Network, Deep Learning, Data Science]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true
---

## Multi-layer perceptron
다음 알아볼 개념은 `순전파`, 그리고 `역전파`에 대해서다.
학습이란 개념은 처음에도 말했듯이, 가중치와 편향치에 대해서 오차를 줄여나가는 것이라고 볼 수 있다.
즉, $y=wx+b$라는 수식에서, 임의의 w, b로부터 점점 y값에 가까운 예측을 하도록 w, b의 값을 조정해나가는 것이라고 볼 수 있다.
이는 아래 수식과 같이 정의할 수 있고, $\eta$ 는 학습률을 나타낸다.

$ w \gets w - \eta \frac{\partial C}{\partial w}$

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 +np.exp(-x))

# Input layer
data_x = np.array([2, 4, 3])  # x1, x2, x3
data_y = [0.8, 0.4]  # y1. y2: ground truth
x = data_x.reshape(-1)

# First layer
w1 = np.array([0.7, 0.3, 0.9]) # w11, w21, w31
w2 = np.array([0.2, 0.4, 0.1]) # w12, w22, w32
b1 = -0.2

W = np.array([w1, w2])
b = np.array([b1, b1])
g1x = np.dot(W, x) + b
h1x = sigmoid(g1x)

print("g1 =",g1x) # g1 = [3.9 3.6]
print("h1 =", h1x) # h1 = [0.98015969 0.97340301]
```

```python
# Second layer

w3 = np.array([0.1, 0.5]) # w41, w42
w4 = np.array([0.8, 0.4]) # w51, w52
b2 = 0.5

W_ = np.array([w3, w4])
b_ = np.array([b2, b2])
g2x = np.dot(W_, h1x) + b_
h2x = sigmoid(g2x)

print("g2 =",g2x) # g2 = [1.04484561 1.65151343]
print("h2 =", h2x) # h2 = [0.73978389 0.83909549]
```