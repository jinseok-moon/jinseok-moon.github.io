---
title: "CNN 01 - 퍼셉트론"
excerpt: "CNN 01 - Perceptron"
tags: [Python, Convolutional Neural Network, Deep Learning, Data Science]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true
---
## What is perceptron?

퍼셉트론[<sup id="fn1-back">1</sup>](#fn1)이란 인공신경망에서 사용하는 최소 단위라고 보면 편하다. 입력값에 대하여 가중합, 활성화 함수를 통해 출력값을 얻어내는 모델이다.  
Eq.\ref{eqn:eq1}은 가중합을 나타내며, Eq.\ref{eqn:eq2}는 활성화 함수를 나타내며, 퍼셉트론에서는 활성화함수로 계단함수를 이용한다.

\begin{equation}\label{eqn:eq1} g(x) = \sum_{i} x_iw_i \qquad (x_0=1, w_0=1) \end{equation}

\begin{equation}\label{eqn:eq2} h(x) = \begin{cases} 0, & g(x) \leq 0 \\ 1, & g(x) > 0 \end{cases} \end{equation}

\begin{equation}\label{eqn:eq3} \hat{y} = h(x) \end{equation}

<center>
	<figure> <img src="/Images/Study/cnn01_01.png" alt="Perceptron" id="fig1"/>
        <figcaption>Fig.1 퍼셉트론</figcaption>
    </figure>
</center>

Fig.[1](#fig1)의 왼쪽 부분처럼 g(x)가 한개뿐인 경우, 단층 퍼셉트론 (Single layer perceptron) 이라고 하며, 여러개를 조합한 경우 다층 퍼셉트론 (Multilayer perceptron) 이라고 한다.

## Perceptron in python
```python
# Basic import
import numpy as np

# Input layer
input_data = np.array([[2, 3], [5, 1]])
x = input_data.reshape(-1)

# Weight & bias
# Single layer perceptron
w1 = np.array([2, 1, -3, 3])
w2 = np.array([1, -3, 1, 3])
b1 = 3
b2 = 3

# Weight sum
W = np.array([w1, w2])
b = np.array([b1, b2])
weight_sum = np.dot(W, x) + b

# Output layer
output_data = 1/(1+np.exp(-weight_sum))

print(output_data) # [0.11920292 0.98201379]
```

## References
[<sup id="fn1">1</sup>](#fn1-back) Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386–408. https://doi.org/10.1037/h0042519  