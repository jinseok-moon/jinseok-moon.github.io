---
title: "CUDA 기초"
excerpt: "CUDA Basic"
categories:
  - CUDA
tags: [C/C++, Develop, GPU]
toc: true
toc_sticky: true
toc_label: "On this page"

published: false
use_math: true

date: 2022-05-22
last_modified_at: 2022-05-22
---

## GPU Architecture

우선 GPU의 구조를 알아야 합니다. 다음 그림과 같이, GPU는 `Streaming Multiprocessors (SM)` 로 이루어져있습니다. SM은 레지스터와 캐시를 가지며, `Streaming Processor (SP) / CUDA Core` 라고 불리는 ALU를 32개 가집니다. 

실제 프로그래밍을 할 때는 각 코어가 (b)에서 보이는 것처럼 thread를 하나씩 사용합니다. 32개의 thread가 활용되는데, 이 단위를 `Warp` 라고 합니다. thread가 모인 그룹을 `Block`이라고 하고, block이 모인 구조가 `Grid` 라고 합니다. Block은 SM들에 나누어져 할당됩니다. 정리하면 다음과 같습니다.
- SM에 Block을 할당
- Block 내부에는 복수의 thread가 존재
- SM 내부의 32개의 코어가 32개의 thread를 할당받는 1 Warp 동작

<center>
<figure style="width: 50%"> <img src="/Images/CUDA/1/Typical-NVIDIA-GPU-architecture-The-GPU-is-comprised-of-a-set-of-Streaming.png" alt="Graph Example"/>
<figcaption>GPU 구조. (HERNÁNDEZ et al., 2013)</figcaption>
</figure>
</center>

SM 내부의 메모리 구조는 다음과 같습니다. 레지스터가 있고, L1 cache와 공유하는 shared memory를 가지며, 그와는 별개로 Constant cache (와 Texture cache)를 갖습니다. 이 메모리들은 L2 cache를 거쳐서 GPU의 Global memory와 커뮤니케이션합니다.
<center>
<figure style="width:50%"> <img src="/Images/CUDA/1/GPUMemLevels.png" alt="Graph Example"/>
<figcaption>GPU 메모리 구조. Image by <a href="https://cvw.cac.cornell.edu/GPUarch/memory_levels">Cornell University</a></figcaption>
</figure>
</center>

## [Compute Unified Device Architecture (CUDA)](https://developer.nvidia.com/cuda-toolkit)
CUDA는 NVIDIA에서 만든 GPU용 프로그래밍 언어입니다. NVIDIA 자사 GPU 환경에서 동작하도록 만들어져있습니다. 툴킷으로 제작되어 다양한 곳에 CUDA를 이용할 수 있습니다.
기존의 CPU 프로그래밍과 달리, 많은 코어를 가지고 수행하는 병렬 연산 (Single Instruction Multiple Threads)에 특화되어있습니다.

위에서 살펴본 것처럼, GPU 메모리는 CPU와는 다른 독립적인 공간에 존재하기 때문에, PCIe 를 통한 데이터 교환을 해줘야 합니다. 이 때, PCIe 통신 속도가 연산속도보다 느리기 때문에 데이터 교환을 최소로 해주는 것이 CUDA 프로그래밍의 핵심이라고 할 수 있습니다.

```cpp

__global__ void add(int *d_a, int *d_b, int *d_c, int size)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    d_c[idx] = d_a[idx] + d_b[idx];
}


int *host_a, *host_b, *host_c;
int *dev_a, *dev_b, *dev_c;

int size = 10;
host_a = new int[size];
host_b = new int[size];
host_c = new int[size];

for (int i=0; i<size; i++)
{
    host_a[i] = i;
    host_b[i] = i*10;
}

// Device Memory allocation
cudaMalloc((void**)&dev_a, sizeof(int)*size);
cudaMalloc((void**)&dev_b, sizeof(int)*size);
cudaMalloc((void**)&dev_c, sizeof(int)*size);

// Memory Copy Host To Device
cudaMemcpy(dev_a, host_a, sizeof(int)*size, cudaMemcpyHostToDevice);
cudaMemcpy(dev_b, host_b, sizeof(int)*size, cudaMemcpyHostToDevice);

// Kernel Computation
myadd <<< gridsize, blocksize >>> (dev_a, dev_b, dev_c, size);

// Memory Copy Device To Host
cudaMemcpy(host_c, dev_c, sizeof(int)*size, cudaMemcpyDeviceToHost);

// Memory Free
cudaFree(dev_a);
cudaFree(dev_b);
cudaFree(dev_c);

delete[] host_a;
delete[] host_b;
delete[] host_c;

```

## References
1. HERNÁNDEZ, Moisés, et al. Accelerating fibre orientation estimation from diffusion weighted magnetic resonance imaging using GPUs. PloS one, 2013, 8.4: e61892.