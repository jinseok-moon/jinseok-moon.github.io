---
title: "CUDA 01 - GPU의 구조 및 CUDA 기초"
categories:
  - Parallel Programming
tags: [CUDA, C/C++, GPU]
toc: true
toc_sticky: true
toc_label: "On this page"

published: true
use_math: true

date: 2022-05-22
last_modified_at: 2022-05-22
---

## 감자 농장을 운영해보자
예전에 CPU, 컴퓨터 관련해서 인터넷에서 재밌는 글을 본 적이 있습니다. 컴퓨터 스펙을 옥수수 농장에 비유하던 글이었는데, 문득 GPU, 특히 CUDA 관련해서도 빗대어 표현하면 훨씬 재밌게 공부가 될 것 같아서 이런 글을 작성해봅니다. CUDA 글에서 CPU는 Host / GPU는 Device 라는 표기를 섞어서 쓸 예정입니다.

## GPU 구조
우선 GPU의 구조를 알아야 합니다. 다음 그림과 같이, GPU는 `Streaming Multiprocessors (SM)` 로 이루어져있습니다. SM은 레지스터와 캐시를 가지며, `CUDA Core` 를 여러개 가집니다. 예전에는 1 SM에 8 Cores (그 당시에는 Streaming Processor, SP라고 불렸습니다) 가 있었는데, 기술이 무럭무럭 발전해서 지금은 엄청나게 많이 들어간다고 합니다. 이제는 꽤 예전 세대가 되어버린 `GTX 1080 Ti` 를 예시로 들면, 28개의 SM이 있고, 각 SM에는 128개의 CUDA Core가 있습니다.

실제 프로그래밍을 할 때는 각 코어가 (b)에서 보이는 것처럼 thread를 하나씩 사용합니다. 32개의 Thread가 활용되는데, 이 단위를 `Warp` 라고 합니다. Thread가 모인 그룹을 `Block`이라고 하고, Block이 모인 구조가 `Grid` 라고 합니다. Block은 SM들에 나누어져 할당됩니다. 정리하면 다음과 같습니다.
- SM에 Block을 할당
- Block 내부에는 복수의 thread가 존재
- SM 내부의 코어가 32개의 thread를 할당받는 1 Warp 동작

<center>
<figure style="width: 50%"> <img src="/Images/CUDA/1/Typical-NVIDIA-GPU-architecture-The-GPU-is-comprised-of-a-set-of-Streaming.png" alt="Graph Example"/>
<figcaption>GPU 구조. Image from HERNANDEZ et al. (2013) </figcaption>
</figure>
</center>

SM 내부의 메모리 구조는 다음과 같습니다. 레지스터가 있고, L1 cache와 공유하는 shared memory를 가지며, 그와는 별개로 Constant cache (+ Texture cache)를 갖습니다. 이 메모리들은 L2 cache를 거쳐서 GPU의 Global memory와 커뮤니케이션합니다.

<center>
<figure style="width:50%"> <img src="/Images/CUDA/1/GPUMemLevels.png" alt="Graph Example"/>
<figcaption>GPU 메모리 구조. Image from Cornell University </figcaption>
</figure>
</center>

## 감자농장 알아보기 - [CUDA](https://developer.nvidia.com/cuda-toolkit)
CUDA는 Compute Unified Device Architecture의 줄임말로 NVIDIA에서 만든 GPU용 프로그래밍 언어입니다. NVIDIA 자사 GPU 환경에서 동작하도록 만들어져있습니다. 툴킷으로 제작되어 다양한 곳에 CUDA를 이용할 수 있습니다.
기존의 CPU 프로그래밍과 달리, 많은 코어를 가지고 수행하는 병렬 연산 (Single Instruction Multiple Threads)에 특화되어있습니다.
위에서 살펴본 것처럼, GPU 메모리는 CPU와는 다른 독립적인 공간에 존재하기 때문에, PCIe 를 통한 데이터 교환을 해줘야 합니다. 이 때, PCIe 통신속도가 연산속도보다 느리기 때문에 데이터 교환을 최소로 해주는 것이 CUDA 프로그래밍의 핵심이라고 할 수 있습니다.

이러한 CUDA 프로그래밍을 감자농장에 빗대어서 간단하게 표현해봅시다.  
CPU의 `대륙 (Host)`과 GPU의 `농장 (Device)` 사이에는 바다가 있어서 서로 접근을 못하는 상태입니다. 근데 GPU 농장에는 많은 일꾼들이 지시를 기다리는 중이죠. 이들은 지시를 내리면 성실하게 수행할 수 있습니다. 농장에 감자를 가지고 가서 심으려 보니, 애초에 아직 땅 개간이 안되어 있습니다. 맨 땅에 감자를 심을 수는 없습니다.
- `메모리 할당 (cudaMalloc)` 농장에 전화해서 '감자 10톤을 가져가서 심을예정입니다. 부족함 없이 심을 수 있게 땅을 개간해두십시오.' 라고 합니다. 이 전화를 통해 땅이 마련됩니다.
- `HostToDevice 메모리 복사 (cudaMemcpyHostToDevice)` 이제 감자를 심을 땅이 생겼으니, 헬리콥터에 10톤의 감자를 싣고 가서 감자를 심습니다.
- `연산 (Kernel)` 어느덧 감자가 자라서, 수확할 시기가 되었습니다. 성실한 일꾼들에게 감자 수확을 하도록 지시합니다.
- `DeviceToHost 메모리 복사 (cudaMemcpyHostToDevice)` 농장으로 가서 수확한 감자를 헬리콥터에 실어서 대륙으로 돌아옵니다.
- `메모리 해제 (cudaFree)` 한 해의 농사가 끝나고, 내년에는 좀 뜬금없지만 사과나무를 심으려고 합니다. 근데 땅이 감자용으로 개간되어있어서 이용할 수 없는 땅이 너무 많습니다. 그래서 땅을 다시 원상태로 돌리도록, 감자용으로 개간했던 땅을 뒤엎습니다.

정말 간단하게 표현하면 이렇게 다섯가지의 과정을 거쳐 CUDA 프로그래밍이 완성됩니다. 코드로 나타내면 다음과 같습니다.

```cpp
__global__ void myadd(int *d_a, int *d_b, int *d_c, int size)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    d_c[idx] = d_a[idx] + d_b[idx];
}

int main()
{
    int *host_a, *host_b, *host_c;
    int *dev_a, *dev_b, *dev_c;

    int size = 64;
    host_a = new int[size];
    host_b = new int[size];
    host_c = new int[size];

    for (int i=0; i<size; i++)
    {
        host_a[i] = i;
        host_b[i] = i*10;
    }

    // Device Memory allocation
    cudaMalloc((void**)&dev_a, sizeof(int)*size);
    cudaMalloc((void**)&dev_b, sizeof(int)*size);
    cudaMalloc((void**)&dev_c, sizeof(int)*size);

    // Memory Copy Host To Device
    cudaMemcpy(dev_a, host_a, sizeof(int)*size, cudaMemcpyHostToDevice);
    cudaMemcpy(dev_b, host_b, sizeof(int)*size, cudaMemcpyHostToDevice);

    dim3 gridsize (1, 1, 1);
    dim3 blocksize (64, 1, 1);
    // Kernel Computation
    myadd <<< gridsize, blocksize >>> (dev_a, dev_b, dev_c, size);

    // Memory Copy Device To Host
    cudaMemcpy(host_c, dev_c, sizeof(int)*size, cudaMemcpyDeviceToHost);

    // 0, 11, 22, 33, ...
    for (int i=0; i<size; i++)
        std::cout << host_c[i] << std::endl;
    
    // Memory Free
    cudaFree(dev_a);
    cudaFree(dev_b);
    cudaFree(dev_c);

    delete[] host_a;
    delete[] host_b;
    delete[] host_c;
}
```
## References
1. HERNÁNDEZ, Moisés, et al. Accelerating fibre orientation estimation from diffusion weighted magnetic resonance imaging using GPUs. PloS one, 2013, 8.4: e61892.
2. Understanding GPU Architecture: Memory Levels, Cornell University. <https://cvw.cac.cornell.edu/GPUarch/memory_level>