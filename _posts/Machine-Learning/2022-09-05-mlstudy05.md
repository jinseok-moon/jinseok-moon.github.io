---
title: "머신러닝 공부 05 - 최적화 기법"
categories:
  - Machine Learning
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true

date: 2022-09-05
---

이번 글에서는 기본적인 경사하강법에 더해서 다른 최적화 방법들에 대해 알아보겠습니다. 주로, local minimum 에 대해서 어떻게 극복할 것인지, 어떤 점이 다른지에 대해 이야기하겠습니다. 우선, 하용호님이 깔끔하게 정리해주신 최적화에 대한 자료를 먼저 소개드립니다. 이 자료를 참고하면서 한개씩 차례로 알아봅시다. 이하 편의상 optimizer 에 대해서는 `GD`, `SGD` 와 같이 약어로 표기하겠습니다. 

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/optimizer.png" />
<figcaption> Optimizer 종류. Image by <a href="https://www.slideshare.net/yongho/ss-79607172"> Yongho Ha </a>
</figcaption>
</figure>
</center>

# 경사하강법 (Gradient Descent, GD)
앞선 글에서는 최적화 기법으로 GD 를 사용했습니다. GD 는 모든 데이터를 대상으로 최적화를 수행합니다. 전체를 다 따져서 오차를 줄이는 최적화이기 때문에 정확도는 높지만, 데이터의 개수가 무수히 많다면 한 스텝 나아가는데에 시간이 그만큼 오래 걸리게 되는 단점이 있습니다. 전체 데이터를 (Full) Batch 라는 단위로 정의할 때가 많기 때문에 Batch Gradient Descent 라고도 합니다. 가중치 $\theta$ 에 대해서, GD 를 이용한 최적화는 다음 식과 같은 형태로 정의됐습니다. 

$$
\begin{equation}
\theta = \theta - \eta \nabla_\theta J(\theta)
\end{equation}
$$ 

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/full-batch.png" />
<figcaption> Batch Gradient Descent </figcaption>
</figure>
</center>

# 확률적 경사하강법 (Stochastic Gradient Descent, SGD)
현실적인 측면에서 바라볼 때, GD 를 적용하기에 어려움이 있기 때문에 SGD 가 등장합니다. SGD 의 기본 개념은 `모두 다 보는게 아니라, 적당적당히 살펴보자` 입니다. 정확히는 전체 데이터중에서 무작위로 `한 개` 의 데이터를 뽑아서 최적화하는 방법입니다. 전체를 다 보는게 아니라 하나씩만 고려하기 때문에 속도가 엄청 빠른 대신, 정확도가 떨어집니다. $x^{(i)}, y^{(i)}$ 를 이용한 $\theta$ 에 대한 업데이트를 다음과 같이 나타낼 수 있습니다.

$$ 
\begin{equation}
\theta = \theta - \eta \nabla_\theta J(\theta;x^{(i)};y^{(i)})
\end{equation}
$$ 

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/sgd.png" />
<figcaption> Stochastic Gradient Descent </figcaption>
</figure>
</center>

# 미니배치 경사하강법 (Mini-batch Gradient Descent)
GD 와 SGD 는 너무 극단적입니다. 한개씩 보거나 전체를 다 보거나, 선택지가 이 두가지 뿐이라면 너무나 슬플것입니다. 그래서 우리는 SGD 보다는 많이, 그러나 GD 보다는 적은 데이터를 보면서 최적화를 하는 중도를 택하게 되는데, 그것이 미니배치입니다. 전체 데이터를 full-batch 로 보고, 일정 개수로 이루어진 mini-batch 를 이용해서 학습시키는 것입니다. SGD 와 비슷하지만, $x^{(i)}, y^{(i)}$ 부터 $x^{(i+n)}, y^{(i+n)}$ 까지의 데이터를 사용하게 됩니다.

$$ 
\theta = \theta - \eta \nabla_\theta J(\theta;x^{(i:i+n)};y^{(i:i+n)})
$$ 

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/mini-batch.png" />
<figcaption> Mini-batch Gradient Descent </figcaption>
</figure>
</center>

> Pytorch 에서 주로 SGD 를 사용하곤 하는데, pytorch 의 SGD 는 DataLoader 에서 batch_size 를 정해주기 때문에 사실상 mini-batch 라고 보시면 됩니다. 

# 모멘텀 (Momentum)
GD 를 사용하다보면, 어느 문제점에 봉착합니다. 앞서 살펴본 글에서는 $W$ 의 값을 임의의 값으로 초기화 하여도 global minimum 에 도달할 수 있었습니다. 하지만, 시그모이드 함수에 대해서 MSE 기법을 적용했을때와 같이, local minimum 이 존재하는 문제에 대해서는 최적화 결과가 global minimum 에 도달할 수 없는 경우가 발생합니다.

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/local-minimum.jpg" />
<figcaption> Local Minimum </figcaption>
</figure>
</center>

여기서 모멘텀의 개념을 도입하게 됩니다. 모멘텀 최적화 기법은 현실의 물리에서 관성의 개념을 따왔습니다. 최적화를 공이 굴러가는 것에 비유해보면, 공은 local minimum 에 도달하여도 갑자기 속력이 0 이 되거나 하지 않습니다. 이러한 관성을 최적화에 도입하게 되면, 최적화 과정에서 어느 minimum 값에 도달해도 일단 지나쳐가게 될 것입니다. 이를 점화식으로 나타내면 다음과 같습니다. $\gamma$ 는 모멘텀에 대한 파라미터입니다.

$$
\begin{equation}
\begin{split}
v_{t}  &= \gamma v_{t-1} + \eta \nabla_\theta J(\theta) \\[5pt]
\theta &= \theta - v_t
\end{split}
\end{equation}
$$

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/momentum.jpg" />
<figcaption> Optimization with Momentum </figcaption>
</figure>
</center>

$\gamma$ 가 0 일 경우에는 일반적인 GD 와 동일하게 되며, 일반적으로 0.9 정도의 값을 사용한다고 합니다. 모멘텀을 사용함에 따라 수렴속도도 빨라지고, 왔다갔다 하는 진동도 유의미하게 줄어들게 됩니다.

# 네스테로프 가속 경사 (Nesterov Accelerated Gradient, NAG)
좀 더 스마트한 공을 굴리는 방법으로 NAG 가 제안되었습니다. NAG 의 식은 다음과 같습니다.

$$
\begin{equation}
\begin{split}
v_{t}  &= \gamma v_{t-1} + \eta \nabla_\theta J(\theta- \gamma v_{t-1} ) \\[5pt]
\theta &= \theta - v_t
\end{split}
\end{equation}
$$

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/nag.png" />
<figcaption> Nesterov Update </figcaption>
</figure>
</center>

모멘텀의 경우, 작은 파란 화살표와 같이 먼저 현재의 gradient 를 계산하고, updated accumulated gradient 방향으로 big jump (큰 파란 화살표) 를 수행합니다. NAG 의 경우, 갈색 화살표와 같이 previous accumulated gradient 방향으로 big jump 를 수행한 다음, gradient 를 계산해서 correction 을 수행합니다 (초록 화살표). 이러한 예측 업데이트는 너무 크게 크게 업데이트 되는 것을 방지하고, 반응성을 증가시켜 여러 task 에서의 RNN 성능을 증가시켰습니다.

# AdaGrad (Adaptive Subgradient)
Adagrad 는 learning rate 를 매개변수의 빈도수에 맞게 조정하여 자주 발생하지 않는 매개변수에 대해 더 큰 업데이트를 수행, 빈번한 매개변수에 대해서는 더 작은 업데이트를 수행합니다. 이러한 이유로 sparse 데이터를 처리하는 데 적합합니다.

Adagrad 에서는 앞서 살펴본것과 달리, 각 파라미터 $\theta_i$ 에 대해 타임스텝 $t$ 에서의 learning rate 을 모두 다르게 가져가게 됩니다. 이를 벡터화 해서 $g_{t,i}$ 로 다음과 같이 나타내고, 이를 이용하여 SGD 업데이트를 합니다.

$$
\begin{equation}
\begin{split}
g_{t,i} &= \nabla_{\theta_t} J(\theta_{t,i}) \\[5pt]
\theta_{t+1,i} &= \theta_{t,i} - \eta \cdot g_{t,i}
\end{split}
\end{equation}
$$

또한 추가적으로 Adagrad 는 일반적인 learning rate $\eta$ 를 다음과 같이 수정합니다. $G_t$ 는 대각요소가 각 gradient 의 제곱의 합인 대각행렬입니다. 제곱근 연산이 빠지면 성능이 훨씬 나빠진다고 하네요. $\epsilon$ 은 0으로 나눠지는 경우를 막기 위한 값입니다.

$$
\begin{equation}
\theta_{t+1} = \theta_{t} - \frac{ \eta }{\sqrt{G_{t}+\epsilon}} \odot g_{t}
\end{equation}
$$

Adagrad 의 장점은 일일이 learning rate 을 조정해줄 필요가 없는 것입니다. 하지만 Adagrad 의 주요 약점은 분모에 제곱 기울기가 누적된다는 것입니다. 추가된 모든 항이 양수이므로 누적 합계는 훈련 중에 계속 증가합니다. 이로 인해 학습률이 줄어들고 결국에는 알고리즘이 더 이상 추가 지식을 얻을 수 없을 만큼 극도로 작아집니다. (이 결함을 해결하기 위해 제안된 알고리즘이 Adadelta 입니다만, 이 포스트에서는 별도로 소개하지 않겠습니다)

# RMSprop
RMSprop 은 Geoff Hinton 이 제안한 알고리즘으로, Adagrad 에서 보이는 급격하게 감소하는 학습률을 해결하기 위해 제안되었습니다.

$$
\begin{equation}
\begin{split}
E[g^2]_{t} &= 0.9E[g^2]_{t-1} + 0.1g^2_t \\[5pt]
\theta_{t+1} &= \theta_{t} - \frac{ \eta }{\sqrt{E[g^2]_t+\epsilon}} g_{t}
\end{split}
\end{equation}
$$

# Adam (Adaptive Moment Esitmation)
Adam 은 각 파라미터가 adaptive learning rate 을 갖는 또다른 알고리즘입니다. Adam 도 RMSprop 이나 Adadelta 처럼 과거 제곱 기울기 $v_t$ 의 기하급수적으로 감소하는 평균을 저장하고, 모멘텀과 유사하게 과거 기울기 $m_t$의 기하급수적으로 감소하는 평균을 유지합니다.

$$
\begin{equation}
\begin{split}
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\[5pt]
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2
\end{split}
\end{equation}
$$

편향 수정된 모멘트 추정치를 계산해서 편향을 상쇄합니다.

$$
\begin{equation}
\begin{split}
\hat{m}_t = \frac{m_t}{1-\beta_1^t} \\[5pt]
\hat{v}_t = \frac{v_t}{1-\beta_2^t}
\end{split}
\end{equation}
$$

업데이트 식은 다음과 같이 정리됩니다.

$$
\begin{equation}
\theta_{t+1} = \theta_{t} - \frac{ \eta }{\sqrt{\hat{v}_t+\epsilon}} \hat{m}_t
\end{equation}
$$

이러한 과정을 통해, 실제로 Adam 이 제일 범용적이고 무난한 optimizer 로 사용되게 되었습니다.

# References
1. <https://www.slideshare.net/yongho/ss-79607172>
2. <https://tykimos.github.io/2017/03/25/Fit_Talk>
3. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.