---
title: "머신러닝 공부 02 - 로지스틱 회귀"
categories:
  - Study
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true

date: 2022-07-23
last_modified_at: 2022-07-24
---

## 선형 회귀로는 풀기 어려운 문제
이전 글에서는 선형 회귀에 대해 알아봤습니다. 선형 회귀는 어떤 `가설` 을 세우고, 그에 대한 `비용` 을 계산하고, `경사하강법` 을 이용해서 비용을 줄이는 방향으로 가중치를 업데이트 하는 알고리즘이었습니다. 모든 문제에 대해서 선형 회귀를 이용해서 풀 수 있다면 참 좋을텐데, 현실은 그렇지 못한 경우가 많습니다. 학생들의 공부한 시간과 시험 합격, 불합격 정보가 담긴 데이터가 있다고 가정해봅시다.
    
|$hours(x)$|$result(y)$|
|------|-----|
|0|Fail|
|5|Fail|
|10|Fail|
|15|Fail|
|20|Pass|
|25|Pass|
|30|Pass|
|35|Pass|

합격을 1, 불합격을 0 으로 두고 선형 회귀를 한다면, 다음과 같이 그려질 것입니다. 주어진 데이터로부터 0.5 기준으로 합격/불합격을 나눈다고 했을 때, 판단 기준점은 대략 17.5 시간 정도가 됩니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/mlstudy02-fig1.jpg" alt=""/>
<figcaption>공부 시간과 시험 결과</figcaption>
</figure>
</center>

여기서, 60시간을 공부해서 합격한 학생의 데이터가 추가되었다고 한다면, $Wx$ 의 기울기는 60 시간의 데이터까지 포함하여 학습하게 될 것입니다. 그렇게 되면 전체적으로 기울기가 완만해지게 되고, 기존에 0.5 를 넘었던 20, 25 시간을 공부했던 학생들은 실제로는 합격했는데도 불구하고 예측 데이터상으로는 불합격을 가리키게 됩니다. 물론 이렇게 극단적으로 변하지는 않겠으나, 어느정도는 충분히 일어날 수 있는 문제입니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/mlstudy02-fig2.jpg" alt=""/>
<figcaption>공부 시간과 시험 결과, 데이터가 추가되었을 때의 변동 </figcaption>
</figure>
</center>

## 로지스틱 회귀 (Logistic Regression)
위에서 살펴본 바와 같이 선형 회귀로는 0 과 1 로만 이루어진 결과값을 구분지어 예측하는 데에 어려움이 있었습니다. 이러한 Binary 문제를 푸는 방법으로, `로지스틱 회귀 (Logistic Regression)` 기법이 있습니다. 로지스틱 회귀는 `x` 와 `y` 의 관계를 함수로 나타내는 것은 선형 회귀와 동일하지만, 결과 값에 대한 확률을 구할 수 있는 것이 다릅니다. 결과가 특정 분류로 나눠지기 때문에 일종의 `분류 (classification)` 기법으로 볼 수 있습니다. `x` 의 입력으로부터 각 `y` 의 값이 0~1 사이로 나타나기 때문에, 로지스틱 회귀를 이용하면 시험 점수를 이용한 합격/불합격 예측, 정상/스팸 메일 분류 문제와 같은 Binary 문제에 활용할 수 있습니다. 

선형 회귀에서는 가설 함수로 $wx$ 를 이용하지만, 로지스틱 회귀에서는 `시그모이드 함수 (Sigmoid Function)` 를 이용합니다. $H(x) = sigmoid(wx)$ 의 가설 함수를 사용할 것입니다. 구체적인 함수식은 다음과 같습니다. 시그모이드 함수를 $ \sigma(x) $ 와 같이 나타내기도 합니다. 

$$
\begin{equation}
H(x) = sigmoid(wx) = {1 \over 1 + e^{-(wx)}} = \sigma(wx)
\end{equation}
$$

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/2/sigmoid.jpg" alt=""/>
<figcaption> w값에 따른 그래프 경사도 차이 </figcaption>
</figure>
</center>

여기서부터는 가설에 `bias` 항을 추가해서 $H(x) = \frac{1}{1+e^{-wx+b}}$ 로 나타냅니다. Bias 는 그래프를 좌, 우로 이동시키게 됩니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/2/sigmoid-bias.jpg" alt=""/>
<figcaption> b값에 따른 그래프 이동 </figcaption>
</figure>
</center>

## 비용 함수
(다중) 선형 회귀의 비용함수로는 $C(w) = \frac{1}{N}\sum_N(H(x_i)-y_i)^2$ 와 같은 평균제곱오차 (Mean Square Error, MSE) 기법을 사용했습니다. 이 식은 가중치 $w$ 에 대해 2차제곱으로 이루어지는 식이기 때문에, 결국 어느 지점에서 시작해도 가장 비용이 최소화되는 점을 찾아갈 수 있었습니다.

그러나 시그모이드 함수에 대해서 MSE 기법을 적용했을때의 $C\text{-}W$ 그래프를 그리게 된다면, 아래의 그림과 같이 그려질 것입니다. 이런 모양에서는 시작점에 따라 가장 낮은 값(global minimum)을 찾아가지 못하고 지역적으로 낮은 값(local minimum)에 수렴할 가능성이 있습니다. 이런 그래프를 non-convex 하다고 합니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/2/nonconvex.jpg" alt=""/>
<figcaption> Non-convex Optimization </figcaption>
</figure>
</center>

그래서 시그모이드 함수에 대해서는 아래와 같은 비용 함수를 적용합니다. $y$ 값에 따라 두 가지로 나누어 지는데, 이 식을 하나로 통합하여 나타냅니다.

$$
\begin{align}
C(w) &= 
\begin{cases} -log(H(x)) & \text{if $y=1$} \\[5pt]
-log(1-H(x)) & \text{if $y=0$}
\end{cases} \\[5pt]
C(w) & = -\left[ylog(H(x))- (1-y)log(1-H(x)) \right] \\[5pt]
\end{align}
$$

$y$ 값이 0 인 데이터에 대해서 $H(x)$ 가 0일때 최소값이되고, 1 인 데이터에 대해서는 $H(x)$ 가 1일때 최소값이 되는 것을 알 수 있습니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/2/sigmoid-cost.jpg" alt=""/>
<figcaption> 시그모이드 함수의 비용 함수 </figcaption>
</figure>
</center>

## Pytorch 로 구현하기
로지스틱 회귀에서 사용할 개념은 `가설 H(X)`, `비용함수 C(W)`, `경사하강법` 이 세가지로 정리할 수 있습니다.

$$
\begin{equation}
H(X) = \frac{1}{1+e^{-W^TX+b}}
\end{equation}
$$

$$
\begin{equation}
C(W) = - \frac{1}{m} \sum ylog(H(X))- (1-y)log(1-H(X))
\end{equation}
$$

$$
\begin{equation}
W := W - \alpha \frac{\partial C}{\partial W}
\end{equation}
$$

이제 이 수식들을 pytorch를 이용해서 구현하면 다음과 같습니다.

```python
import torch
import torch.optim as optim

X = torch.Tensor([[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]])
y = torch.Tensor([[0], [0], [0], [1], [1], [1]])
W = torch.zeros((2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)  # bias 추가
optimizer = optim.SGD([W, b], lr=1)
num_epoch = 1000

for epoch in range(num_epoch+1):
    h = torch.sigmoid(torch.matmul(X, W)+b)
    cost = -torch.mean(y*torch.log(h)+(1-y)*torch.log(1-h))

    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print(f'Epoch {epoch}/{num_epoch} Cost: {cost.data}')


print("--- Final Prediction ---")
final_h = torch.sigmoid(torch.matmul(X, W)+b)
prediction = final_h >= 0.5
print(prediction)
```

학습에 따라 Cost 가 낮아지는 것을 확인할 수 있고, 또 최종적으로 학습된 모델을 이용했을 때, 각 $y$ 값을 잘 예측하는 것을 볼 수 있습니다.

```bash
Epoch 0/1000 Cost: 0.6931471824645996
Epoch 100/1000 Cost: 0.13472206890583038
Epoch 200/1000 Cost: 0.08064315468072891
Epoch 300/1000 Cost: 0.05790002644062042
Epoch 400/1000 Cost: 0.0452997200191021
Epoch 500/1000 Cost: 0.037260960787534714
Epoch 600/1000 Cost: 0.03167250379920006
Epoch 700/1000 Cost: 0.02755594812333584
Epoch 800/1000 Cost: 0.024394338950514793
Epoch 900/1000 Cost: 0.02188830077648163
Epoch 1000/1000 Cost: 0.019852163270115852
--- Final Prediction ---
tensor([[False],
        [False],
        [False],
        [ True],
        [ True],
        [ True]])
```