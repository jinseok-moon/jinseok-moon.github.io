---
title: "머신러닝 공부 04 - 정규화"
categories:
  - Study
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true

date: 2022-07-31
last_modified_at: 2022-07-31
---

## 정규화 (Regularization)

앞서 살펴보았던 함수식들에서, $H(x)$ 의 복잡도에 따라, 회귀 모델이 보이는 양상은 세가지로 분류할 수 있습니다. 너무 단순한 경우에는 입력값에 대해서 예측이 잘 되지 않는 underfit이 발생하며, 반대로 너무 복잡하면 학습 데이터에만 딱 들어맞는 overfit이 발생합니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/4/fits.png" />
<figcaption>모델이 보이는 양상</figcaption>
</figure>
</center>

이러한 문제점을 해결하고 모델의 정확도를 높이기 위해, $C(W)$에 페널티항을 추가하여 계산하는 것이 `정규화 (Regularization)` 입니다. 정규화의 방법은 크게 두가지로, `LASSO (Least Absolute Shrinkage and Selection Operator)` 정규화와, `Ridge` 정규화가 있습니다. 이 둘은 L1, L2 regularization 이라고도 불립니다. 각 정규화에서 부여하는 페널티항은 아래 식과 같이 정의됩니다. $\lambda$ 는 regularization parameter 입니다.

$$
\begin{align}
J_{L1}(\theta) &= \frac{1}{2n}\sum_{i=1}^n(h(x^{(i)})-y^{(i)})^2 + \frac{\lambda}{2}\sum_{j=1}^n|\theta_j| \\
J_{L2}(\theta) &= \frac{1}{2n}\sum_{i=1}^n(h(x^{(i)})-y^{(i)})^2 + \frac{\lambda}{2}\sum_{j=1}^n\theta_j^2
\end{align}
$$

LASSO 정규화는 단순 더하기 연산으로써, 파라미터를 업데이트할때 상수값을 빼주게 되고, 결국 특정 파라미터는 0 이 됩니다. 이 의미는 위의 그림의 overfit 경우를 예로들자면, $H(x)$ 에서 $\theta_3, \theta_4$ 의 값이 작아져서 0 이 되고, 결과적으로 $x^3, x^4$ 의 영향이 없어져서 예측값이 정확해지는 것을 의미합니다. LASSO 정규화는 이러한 feature selection 이 가능한 특징이 있습니다.

<center>
<figure style="width: 30%"> <img src="/Images/Study/mlstudy/4/lasso.jpg" alt="LASSO regularization" id="lasso"/>
<figcaption>LASSO (L1) 정규화</figcaption>
</figure>

<figure style="width: 30%"> <img src="/Images/Study/mlstudy/4/ridge.jpg" alt="Ridge regularization" id="ridge"/>
<figcaption>Ridge (L2) 정규화</figcaption>
</figure>
</center>

LASSO 정규화를 2차원상에서 기하학적으로 보면, $\beta_1, \beta_2$ 파라미터에 대해서  L1 norm 을 제한하였을때의 구간은 마름모꼴로 나타나게 되고, 제한된 구간 내에서 가장 최소의 에러를 가지는 점은, 타원 그래프와의 접점이 됩니다. 이 타원 그래프는 같은 에러인 점들을 모은 그래프입니다. $\beta_1=0$ 인 점에서 접점이 되는 것을 통해 feature selection 또한 알 수 있습니다.

Ridge 정규화의 경우, 페널티항으로 인해 업데이트시에 파라미터 값을 0이 되지는 않는 선에서 아주 작아질 수도 있게 되어 전체적인 성능을 조정하여 정확도를 높이는 방식입니다. 마찬가지로 기하학적으로 보면, $\beta_1, \beta_2$ 파라미터에 대해서 L2 norm 을 제한하였을때의 구간은 원형으로 나타나게 되고, 이 경우에서는 타원과의 그래프의 접점이 회귀 결과값이 됩니다. 

# References
1. A. Ng, "CS229 Lecture notes", vol. 1, no. 1, pp. 3–11, 2000.
2. GeeksforGeeks, Underfitting and Overfitting, [Link](https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/)
3. R. Sneiderman, "From linear regression to ridge regression, the lasso, and the elastic net", [Link](https://towardsdatascience.com/from-linear-regression-to-ridge-regression-the-lasso-and-the-elastic-net-4eaecaf5f7e6)