---
title: "머신러닝 공부 01 - 선형 회귀"
categories:
  - Study
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true

date: 2022-07-18
last_modified_at: 2022-07-18
---

## 선형 회귀 (Linear Regression)
선형 회귀란 어떠한 현상을 가장 잘 나타내는 함수식을 찾는 과정입니다. 이러한 함수식을 가설(Hypothesis)이라고 하며, 주로 $$H(x)$$ 로 나타냅니다. 

$$
\begin{equation}
H(x) = wx
\end{equation}
$$

$$H(x) = wx$$ 라는 간단한 가설을 세웠습니다. 이 식을 풀어서 정의하자면 $$y$$ 에 대해서 $$x$$ 가 가지는 영향력을 $$w$$ 라는 가중치로 나타낸 것과 같습니다. 여기서 $$x=1, y=2$$ 인 무척 단순한 경우를 생각해보겠습니다. 우리는 $$2=w \cdot 1$$ 에서, $$w$$의 값이 2 라는 사실을 알지만, 컴퓨터는 이를 알지 못합니다. 즉, 우리는 $$w$$ 의 값을 2 에 보다 가깝게 맞춰주는 `최적화` 작업을 하게 되고, 이 과정이 바로 `학습` 이라고 할 수 있습니다.

앞서 정의한 $$H(x)$$ 에 대해서 가중치 $$w$$의 초기값이 0 인 경우를 생각해보겠습니다. 실제값은 2 인데, $$H(x)=wx=0 \cdot 1=0$$ 로 예측값과 차이가 발생했습니다. 실제값과 예측값의 차이를 통해, 가중치를 어떻게 갱신해줘야 이 차이를 줄이고 보다 정확해질 수 있을지를 생각합니다. 여기서 사용되는 개념이 `비용함수`, `손실함수` 입니다. 비용함수를 다음과 같이 정의해보겠습니다. 비용함수를 적용하면 cost는 4 가 나오게 되고, cost를 최소화 하는 것을 통해 가중치를 2 에 맞춰가는 최적화를 수행하게 됩니다. 이러한 비용-가중치($$w$$) 의 관계를 그래프로 나타내면 아래와 같습니다.

$$
\begin{equation}
C(w) = (wx-y)^2
\end{equation}
$$

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/cost.jpg" alt=""/>
<figcaption>가중치와 비용의 관계</figcaption>
</figure>
</center>

## Gradient Descent Algorithm
그렇다면, 이런 최적화는 어떻게 할 수 있을까요? 어떻게하면 비용을 최소화하여 실제값에 근접하게 예측할 수 있을까요? 여기서 머신러닝에서 기본 바탕이 되는 개념인 `경사하강법 Gradient Descent Algorithm` 이 나옵니다. 경사하강법에서는 함수식에서 기울기가 완만해지는 방향으로, 비용함수에 대해 해당 지점에서의 미분을 통해 얻어지는 `기울기`를 이용해 최적화를 수행합니다. $$w$$ 의 업데이트를 수식으로 나타내면 다음과 같습니다. $$\eta$$ 는 보폭(step size)을 의미하고, 이는 한번의 계산으로 얼만큼 갱신할지를 정하는 파라미터입니다. 다른 말로는 학습률(learning rate, lr)이라고도 하며, 이 예제에서는 0.05 로 설정했습니다.

$$
\begin{equation}
\frac{\partial C}{\partial w} = 2(wx-y)x
\end{equation}
$$

$$
\begin{equation}
w := w - \eta \frac{\partial C}{\partial w}
\end{equation}
$$

미분을 통해, $$w$$ 가 0 일때 얻어진 기울기는 -4 로, 다음 $$w$$ 의 값은 $$w=0-0.05 \cdot (-4)$$ 인 0.2 가 됩니다. 갱신된 가중치를 이용해서 다시금 계산해보면, $$ H(x) = 0.2 \cdot 1 = 0.2 $$ 가 되고, 그에 대한 $$C(w)$$ 는 $$(0.2-2)^2=2.592$$ 로, 아까보다 비용이 줄어든 것을 확인해 볼 수 있습니다. 초기 가중치를 0으로 시작해서, 경사하강법으로 최적화를 수행하면 아래의 그림과 같이 점점 cost가 가장 낮아지는 방향으로 $$w$$ 의 값이 갱신됩니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/gd-lr0.05.jpg" alt=""/>
<figcaption>경사하강법 (lr=0.05)</figcaption>
</figure>
</center>

위에서 잠깐 나왔던 `보폭 (step size)` 을 변경한다면 어떻게 될까요? 아래 그림과 같이 매우 움직임이 큰 것을 알 수 있습니다. 큰 학습률을 사용하게 되면 비용이 적어지는 쪽으로 움직이는 것이 아니라 오히려 비용이 커지는 쪽으로 움직여서 발산할 가능성이 있으니, 자신의 모델 및 데이터에 맞게 학습률을 정하는 것도 중요한 작업입니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/gd-lr0.85.jpg" alt=""/>
<figcaption>경사하강법 (lr=0.85)</figcaption>
</figure>
</center>

아까전의 Gradient Descent 수식을 보면, $$ w \gets w - \eta \frac{\partial C}{\partial w} $$ 로, 기존값에서 빼주게 됩니다. 이는 현재 지점에서의 기울기의 절대값이 줄어드는 방향으로 만들어 주기 위함입니다. 실제로 현재 지점에서의 기울기가 음수라면 마이너스 연산으로 인해 음의 기울기에 양수를 더해주어 작아지게 되고, 반대의 경우라면 양의 기울기에 양수를 빼주게 되어 작아지게 됩니다. 

## 다중 선형 회귀 (Multi-variable Linear Regression)
앞에서는 `y` 에 대해 변수가 `x` 하나밖에 없는 선형회귀에 대해서 알아보았습니다. 하지만 실제 환경에서는 변수를 많이 가지고 있을 것입니다. $$x_1, x_2, x_3$$ 에 대해서 각각 가중치 $$w_1, w_2, w_3$$ 을 가지는 $$H(x)$$ 를 다음과 같이 정의해보겠습니다. 이 식은 다음 식과 같이 묶을 수 있고, 행렬곱으로도 표현할 수 있습니다. 머신러닝 분야에서는 행렬곱으로 표현하는 것이 일반적입니다.

$$
\begin{equation}
H(x) = w_1x_1 + w_2x_2 + w_3x_3 = \sum_{i=1}^3 w_ix_i
\end{equation}
$$

$$
\begin{equation}
H(x) = \left( \begin{matrix}  x_1 & x_2 & x_3  \end{matrix} \right) \left( \begin{matrix}  w_1 \newline w_2 \newline w_3 \end{matrix} \right)
\end{equation}
$$

$$
\begin{equation}
H(X) = XW
\end{equation}
$$

|$$x_1$$|$$x_2$$|$$x_3$$|$$y$$|
|------|:---:|:---:|:-:|
|1|2|3|10|

인 케이스를 한번 생각해보겠습니다. $$w_1, w_2, w_3$$ 은 모두 초기값을 0으로 주었습니다. 아까와 같이, 비용함수 $$C(W)=(H(X)-y)^2$$ 를 정의하여서 최적화를 수행해봅시다. 초기값에 대한 비용은 $$(0-10)^2=100$$ 이 나옵니다. 다변수에 대해서 경사하강법을 적용하려면 각 가중치에 대해서 편미분을 수행하면 됩니다. 즉, $$w_1$$ 에 대해서는 다음과 같은 수식을 풀게 됩니다. $$w_2, w_3$$ 을 상수로 취급했기 때문에 미분을 통해 남은 값인 $$x_1$$ 만 곱해지게 됩니다.

$$
\begin{equation}
w_1 \gets w_1 - \eta \frac{\partial C}{\partial w_1}
\end{equation}
$$

$$
\begin{equation}
\frac{\partial C}{\partial w_1} = 2(w_1x_1+w_2x_2+w_3x_3-y)x_1
\end{equation}
$$

|$$w_1$$|$$w_2$$|$$w_3$$|$$cost$$|
|------|:---:|:---:|:-:|
|1|2|3|16|

$$w_2, w_3$$ 에 대해서도 동일하게 계산을 수행한 결과 가중치가 1, 2, 3 으로 갱신되었고, 차이가 작아졌습니다! 
이렇게 단일변수가 아닌 다변수를 가지는 가설에 대해서도 경사하강법을 동일하게 적용 가능함을 알 수 있었습니다.

## [Pytorch 를 이용한 자동미분](https://tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial.html)
단일변수, 다변수 상관없이 경사하강법을 이용하여 선형 회귀 가설식을 성공적으로 최적화 할 수 있게 되었습니다. 하지만 이런 수식에 대해서 매번 미분을 통해 가중치를 업데이트 해주는 일은 무척이나 번거로운 일임에 틀림없습니다. 이에 더해, 변수가 100개정도 된다면 한 번 계산하는데에 시간이 오래 걸릴 것입니다. 이러한 문제점을 해결하기 위해 tensorflow, pytorch 등의 프레임워크는 각 연산을 기억해 두었다가, 역방향으로 각 연산의 변화량(기울기)을 구하는 `자동 미분 (autograd)` 기능을 지원합니다. 위에서 살펴본 다변수 선형 회귀를 pytorch 를 이용해서 풀면 아래와 같은 결과를 얻을 수 있습니다.

```python
import torch
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

X = torch.Tensor([1., 2., 3.])
y = torch.Tensor([10.])
W = torch.zeros(3, requires_grad=True)

optimizer = optim.SGD([W], lr=0.05)
# gradient를 0으로 초기화
num_epoch = 5

for epoch in range(num_epoch+1):
    h = torch.sum(W * X)  # h = w1x1 + w2x2 + w3x3
    cost = (h - y) ** 2

    # zero_grad 호출 필요!
    optimizer.zero_grad()
    # 비용 함수를 미분하여 gradient 계산
    cost.backward()
    # W를 업데이트
    optimizer.step()
    print(f'Epoch {epoch}/{num_epoch} W: {W.data}, Cost: {cost.data}')
```

```bash
Epoch 0/5 W: tensor([1., 2., 3.]), Cost: tensor([100.])
Epoch 1/5 W: tensor([0.6000, 1.2000, 1.8000]), Cost: tensor([16.])
Epoch 2/5 W: tensor([0.7600, 1.5200, 2.2800]), Cost: tensor([2.5600])
Epoch 3/5 W: tensor([0.6960, 1.3920, 2.0880]), Cost: tensor([0.4096])
Epoch 4/5 W: tensor([0.7216, 1.4432, 2.1648]), Cost: tensor([0.0655])
Epoch 5/5 W: tensor([0.7114, 1.4227, 2.1341]), Cost: tensor([0.0105])
```

살펴본 바와 같이 cost 가 점점 작아지는 방향으로 가중치가 갱신되었습니다. 
또한, 위의 코드를 보면 `optimizer.zero_grad()` 라는 구문이 있습니다. Pytorch 는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있기 때문에, zero_grad() 함수를 통해 미분값을 0 으로 초기화 시켜줄 필요가 있습니다. 초기화를 시켜주지 않으면 다음과 같은 결과를 얻고, 이는 올바르게 학습하지 못하는 이유가 됩니다.

```python
import torch
w = torch.tensor(2.0, requires_grad=True)

nb_epochs = 5
for epoch in range(nb_epochs + 1):
  z = 2*w
  z.backward()
  print(f'{epoch}, Grad: {w.grad}')
```

```bash
0, Grad: 2.0
1, Grad: 4.0
2, Grad: 6.0
3, Grad: 8.0
4, Grad: 10.0
5, Grad: 12.0
```