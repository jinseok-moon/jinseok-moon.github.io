---
title: "머신러닝 공부 06 - 퍼셉트론"
categories:
  - Study
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true

date: 2022-09-07
---

# 퍼셉트론
이번 글에서는 퍼셉트론에 대해 알아보겠습니다. 퍼셉트론은 뉴런을 모사하여 나타낸 초기 형태의 인공 신경망입니다. 입력값에 대하여 가중치와 편향치를 가지고, 활성화 함수를 통해 출력값을 얻어내는 모델입니다. 

퍼셉트론에서는 활성화 함수로 계단함수를 사용하였는데, 이 대신 시그모이드 함수 혹은 소프트맥스 함수를 사용하게 되면 앞서 공부한 회귀 모델과 동일함을 알 수 있습니다.

$$
\begin{equation}
\begin{split}
 g(x) &= \sum^{n}_{i} x_iw_i + b \\[5pt]
h(x) &= \begin{cases} 0, & g(x) \leq 0 \newline 1, & g(x) > 0 \end{cases} \\[5pt]
\hat{y} &= h(x)
\end{split}
\end{equation}
$$


# 단층 퍼셉트론 (Single Layer Perceptron)
퍼셉트론을 한개만 사용한 단층 퍼셉트론이라고 합니다. 당시의 사람들은 이 단층 퍼셉트론을 이용해 논리 게이트들을 구현하였습니다. 들어가는 층을 `input layer`, 나오는 결과 층을 `output layer` 라고 합니다.

AND 는 양 값이 모두 1 일때 1 을 출력하는 게이트로, 그래프로 나타내면 다음과 같습니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/6/andgraphgate.png" />
<figcaption>AND gate</figcaption>
</figure>
</center>

단층 퍼셉트론의 $w, b$ 를 적절히 조절해줌으로써 위와 같은 논리 게이트를 구현할 수 있고, OR 과 NAND 게이트에 대해서도 마찬가지로 구현할 수 있습니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/6/oragateandnandgate.png" />
</figure>
</center>

하지만 단층 퍼셉트론으로는 직선으로밖에 표현할 수 없기 때문에, XOR 게이트를 구현할 수 없는 한계가 있습니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/6/xorgraphandxorgate.png" />
</figure>
</center>


# 다층 퍼셉트론 (Multi Layer Perceptron)
퍼셉트론은 인공 신경망의 개념으로 등장하였으나, 단층으로는 단순한 XOR 문제도 풀지 못했고, 복수의 퍼셉트론을 조합한 다층 퍼셉트론을 이용해야 합니다. 통상적으로 MLP 라고 부릅니다.

다층 퍼셉트론을 이용해서 곡선으로 영역을 구분지어 풀 수 있습니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/6/xorgate_nonlinearity.png" />
</figure>
</center>

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/6/perceptron.png" />
</figure>
</center>

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/6/in-hidden-out.png" />
<figcaption>Multi Layer Perceptron</figcaption>
</figure>
</center>

MLP 은 단층 퍼셉트론의 input / output layer 의 사이에 `hidden layer` 가 추가됩니다. 그림에서 hidden layer**s** 로 표현해 두었듯이, 사이에 추가될 수 있는 은닉층의 개수는 제한이 없습니다.

여담으로, Marvin Minsky 와 Seymour Papert 는 단층 퍼셉트론만으로는 XOR 게이트를 풀지 못하고, 다층 퍼셉트론을 사용해야 풀 수 있다는 내용을 수학적으로 증명하였습니다. 이들의 책 `Perceptrons` 에서는 책 제목처럼 퍼셉트론을 구체적으로 서술하는데, 다층 퍼셉트론을 사용하면 XOR 게이트를 풀 수 있고, 다른 것들도 풀 수 있다고 하였습니다. 하지만 간단한 함수더라도 이를 풀기 위한 적절한 $w, b$ 값을 찾아내는 것은 현실적으로 불가능하다고 주장하였고, 그러한 분위기 속에서 한동안 신경망 연구에 대한 열기가 급격하게 식었습니다. 그러나 이제 우리는 어떻게 적절한 값을 찾아내는지 알고 있죠?


## References
1. <https://wikidocs.net/60680>
2. <https://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7>