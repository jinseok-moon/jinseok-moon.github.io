---
title: "머신러닝 공부 01 - 선형회귀"
categories:
  - Study
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true

date: 2022-07-13
last_modified_at: 2022-07-13
---

## 선형회귀 (Linear Regression)
선형회귀란 어떠한 현상을 가장 잘 나타내는 함수식을 찾는 과정이라고 생각하시면 됩니다. 이러한 함수식을 가설(Hypothesis)이라고 하며, 주로 $$h(x)$$로 나타냅니다. 

$$
\begin{equation}
h(x) = wx
\end{equation}
$$

$$h(x) = wx$$ 라는 간단한 가설을 세웠습니다. 이 식을 풀어서 정의하자면 $$y$$에 대해서 $$x$$가 가지는 영향력을 $$w$$ 라는 가중치로 나타낸 것과 같습니다. 여기서 $$x=1, y=2$$ 인 무척 단순한 경우를 생각해보겠습니다. 우리는 $$2=w \cdot 1$$ 에서, $$w$$의 값이 2라는 사실을 알지만, 컴퓨터는 이를 알지 못합니다. 즉, 우리는 $$w$$ 의 값을 2에 보다 가깝게 맞춰주는 `최적화` 작업을 하게 되고, 이 과정이 바로 `학습` 이라고 할 수 있습니다.

먼저, 앞서 정의한 $$h(x)$$ 에 대해서, 가중치 $$w$$의 초기값이 0인 경우를 생각해보겠습니다. 실제값은 2인데, $$h(x)=wx=0 \cdot 1=0$$ 으로 예측값과 차이가 발생했습니다. 실제값과 예측값의 차이를 통해, 가중치를 어떻게 해줘야 이 차이를 줄일 수 있을지를 생각합니다. 여기서 사용되는 개념이 `비용함수`, `손실함수` 등의 개념입니다. 비용함수를 다음과 같이 정의해보겠습니다. 비용함수를 적용하면 cost는 4가 나오게 되고, cost를 최소화 하는 것을 통해 가중치를 2에 맞춰가는 최적화를 수행하게 됩니다. 이러한 비용-가중치($$w$$)의 관계를 그래프로 나타내면 아래와 같습니다.

$$
\begin{equation}
C(w) = (wx-y)^2
\end{equation}
$$

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/cost.jpg" alt=""/>
<figcaption>가중치와 비용의 관계</figcaption>
</figure>
</center>

## Gradient Descent Algorithm
그렇다면, 이런 최적화는 어떻게 할 수 있을까요? 여기서 머신러닝에서 기본 바탕이 되는 개념인 `경사하강법 Gradient Descent Algorithm` 이 나옵니다. 경사하강법에서는 함수식에서 기울기가 완만해지는 방향으로,  비용함수에 대해 해당 지점에서의 미분을 통해 얻어지는 `기울기`를 이용해 최적화를 수행합니다. $$w$$의 업데이트를 수식으로 나타내면 다음과 같습니다.$$\eta$$ 는 학습률을 나타내는데, 이는 한번의 계산으로 얼만큼 갱신할지를 정하는 파라미터입니다. 이 예제에서는 0.05로 설정했습니다. 

$$
\begin{equation}
\frac{\partial C}{\partial w} = 2(wx-y)x
\end{equation}
$$

$$
\begin{equation}
w \gets w - \eta \frac{\partial C}{\partial w}
\end{equation}
$$

미분을 통해, $$w$$ 가 0일때 얻어진 기울기는 -4 로, 다음 $$w$$ 의 값은 $$w=0-0.05 \cdot (-4)$$ 인 0.2 가 됩니다. 갱신된 가중치를 이용해서 다시금 계산해보면, $$ h(x) = 0.2 \cdot 1 = 0.2 $$ 가 되고, 그에 대한 $$C(w)$$ 는 $$(0.2-2)^2=2.592$$ 로, 아까보다 비용이 줄어든 것을 확인해 볼 수 있습니다. 초기 가중치를 0으로 시작해서, 경사하강법으로 최적화를 수행하면 아래의 그림과 같이 점점 cost가 가장 낮아지는 방향으로 $$w$$ 의 값이 갱신됩니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/gd-lr0.05.jpg" alt=""/>
<figcaption>경사하강법 (lr=0.05)</figcaption>
</figure>
</center>

위에서 잠깐 나왔던 `학습률 (learning rate)` 을 변경한다면 어떻게 될까요? 옆으로 매우 움직임이 큰 것을 알 수 있는데, 큰 학습률을 사용하게 되면 비용이 적어지는 쪽으로 움직이는 것이 아니라 오히려 비용이 커지는 쪽으로 움직여서 발산할 가능성이 있으니, 자신의 모델 및 데이터에 맞게 학습률을 정하는 것도 중요한 작업입니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/gd-lr0.85.jpg" alt=""/>
<figcaption>경사하강법 (lr=0.85)</figcaption>
</figure>
</center>

아까전의 식(3)을 보면, $$ w \gets w - \eta \frac{\partial C}{\partial w} $$ 로, 기존값에서 빼주게 됩니다. 이는 현재 지점에서의 기울기의 절대값이 줄어드는 방향으로 만들어 주기 위함입니다. 실제로 현재 지점에서의 기울기가 음수라면 마이너스 연산으로 인해 음의 기울기에 양수를 더해주어 작아지게 되고, 반대의 경우라면 양의 기울기에 양수를 빼주게 되어 작아지게 됩니다. 

전체 코드는 다음과 같습니다.

```python
import numpy as np
import matplotlib.pyplot as plt

# Plot setting
plt.rcParams["figure.figsize"] = (6.4,6.4)
plt.rcParams['axes.grid'] = False 

x = 1.
y = 2.
wlist = np.linspace(0,4, 50)
clist = []
updatew = []
updatec = []
w = 0

for _w in wlist:
    h = _w*x
    cost = (h-y)**2
    clist.append(cost)

lr = 0.05
num_epoch = 30
for epoch in range(num_epoch):
    h = w*x
    c = (h-y)**2
    updatec.append(c)
    updatew.append(w)
    dcdw = 2*(w*x-y)*x
    w = w - lr * dcdw

fig, ax = plt.subplots()
ax.plot(wlist, clist)
for n in range(num_epoch-1):
    ax.plot([updatew[n], updatew[n+1]], [updatec[n], updatec[n+1]], color='C1', marker=".")
ax.set_ylabel("Cost")
ax.set_xlabel("$w$")
plt.show()
```