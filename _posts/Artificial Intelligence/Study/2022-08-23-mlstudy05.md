---
title: "머신러닝 공부 05 - 최적화"
categories:
  - Study
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: false
use_math: true

date: 2022-08-23
---

# 최적화 (Optimization)
앞선 글에서 회귀 모델을 공부하면서 동시에 최적화의 기법중 하나인 경사하강법, Gradient Descent 에 대해 알아보았습니다. 이번 글에서는 경사하강법 외의 다른 최적화 방법에 대해 알아보겠습니다.

가중치 $w$ 에 대해서, 경사하강법을 이용한 최적화는 $ W := W - \alpha \frac{\partial C}{\partial W} $ 와 같은 형태로 정의됐습니다. 이 식을 풀어쓰면 다음과 같게 되겠죠.

$$ 
W := W - \alpha \frac{1}{m}\sum^m_{i=1}(Wx^{(i)}-y^{(i)})x^{(i)} 
$$ 



<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/2/nonconvex.jpg" alt=""/>
<figcaption> Non-convex Optimization </figcaption>
</figure>
</center>

로지스틱 회귀 문제를 풀 때, 단순한 경

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/optimizer.png" />
<figcaption> Optimizer 종류. Image by <a href="https://www.slideshare.net/yongho/ss-79607172"> Yongho Ha </a>
</figcaption>
</figure>
</center>

