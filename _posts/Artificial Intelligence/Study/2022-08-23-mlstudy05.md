---
title: "머신러닝 공부 05 - 최적화"
categories:
  - Study
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: false
use_math: true

date: 2022-08-23
---

이번 글에서는 경사하강법 외의 다른 최적화 방법에 대해 알아보겠습니다. 주로, local minimum 에 대해서 어떻게 극복할 것인지, 경사하강법과는 무엇이 다른지에 대해 이야기하겠습니다. 우선, 하용호님이 깔끔하게 정리해주신 최적화에 대한 자료를 먼저 소개드립니다. 이 자료를 참고하면서 한개씩 차례로 알아봅시다. 이하 편의상 optimizer 에 대해서는 `GD`, `SGD` 와 같이 약어로 표기하겠습니다. 

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/optimizer.png" />
<figcaption> Optimizer 종류. Image by <a href="https://www.slideshare.net/yongho/ss-79607172"> Yongho Ha </a>
</figcaption>
</figure>
</center>

# 경사하강법 (Gradient Descent, GD)
앞선 글에서는 최적화 기법으로 GD 를 사용했습니다. 가중치 $w$ 에 대해서, GD 를 이용한 최적화는 $ W := W - \alpha \frac{\partial C}{\partial W} $ 와 같은 형태로 정의됐습니다. 이 식을 풀어쓰면 다음 식과 같은 형태로 됩니다.

$$ 
W := W - \alpha \frac{1}{m}\sum^m_{i=1}(Wx^{(i)}-y^{(i)})x^{(i)} 
$$ 

GD 는 모든 데이터를 대상으로 최적화를 수행합니다. 전체를 다 따져서 오차를 줄이는 최적화이기 때문에 정확도는 높지만, 데이터의 개수가 무수히 많다면 한 스텝 나아가는데에 시간이 그만큼 오래 걸리게 되는 단점이 있습니다.

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/full-batch.png" />
<figcaption> Full-batch Gradient Descent </figcaption>
</figure>
</center>

# 확률적 경사하강법 (Stochastic Gradient Descent, SGD)
현실적인 측면으로 GD 를 적용하기 어렵기 때문에, SGD 가 등장합니다. SGD 의 기본 개념은 `모두 다 보는게 아니라, 적당적당히 살펴보자` 입니다. 정확히는 전체 데이터중에서 무작위로 `한 개` 의 데이터를 뽑아서 최적화하는 방법입니다. 전체를 다 보는게 아니라 하나씩만 고려하기 때문에 속도가 엄청 빠른 대신, 정확도가 떨어집니다. 

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/sgd.png" />
<figcaption> Stochastic Gradient Descent </figcaption>
</figure>
</center>

# 미니배치 경사하강법 (Mini-batch Gradient Descent)
GD 와 SGD 는 너무 극단적입니다. 한개씩 보거나 전체를 다 보거나, 선택지가 이 두가지 뿐이라면 너무나 슬플것입니다. 그래서 우리는 SGD 보다는 많이, 그러나 GD 보다는 적은 데이터를 보면서 최적화를 하는 중도를 택하게 되는데, 그것이 미니배치입니다. 전체 데이터를 full-batch 로 보고, 일정 개수로 이루어진 mini-batch 를 이용해서 학습시키는 것입니다.

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/mini-batch.png" />
<figcaption> Mini-batch Gradient Descent </figcaption>
</figure>
</center>

> Pytorch 에서 주로 SGD 를 사용하곤 하는데, pytorch 의 SGD 는 DataLoader 에서 batch_size 를 정해주기 때문에 사실상 mini-batch 라고 보시면 됩니다. 

# 

# References
1. <https://www.slideshare.net/yongho/ss-79607172>
2. <https://tykimos.github.io/2017/03/25/Fit_Talk>