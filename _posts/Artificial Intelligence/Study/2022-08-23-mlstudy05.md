---
title: "머신러닝 공부 05 - 최적화 기법"
categories:
  - Study
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true

date: 2022-08-23
---

이번 글에서는 기본적인 경사하강법에 더해서 다른 최적화 방법들에 대해 알아보겠습니다. 주로, local minimum 에 대해서 어떻게 극복할 것인지, 어떤 점이 다른지에 대해 이야기하겠습니다. 우선, 하용호님이 깔끔하게 정리해주신 최적화에 대한 자료를 먼저 소개드립니다. 이 자료를 참고하면서 한개씩 차례로 알아봅시다. 이하 편의상 optimizer 에 대해서는 `GD`, `SGD` 와 같이 약어로 표기하겠습니다. 

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/optimizer.png" />
<figcaption> Optimizer 종류. Image by <a href="https://www.slideshare.net/yongho/ss-79607172"> Yongho Ha </a>
</figcaption>
</figure>
</center>

# 경사하강법 (Gradient Descent, GD)
앞선 글에서는 최적화 기법으로 GD 를 사용했습니다. GD 는 모든 데이터를 대상으로 최적화를 수행합니다. 전체를 다 따져서 오차를 줄이는 최적화이기 때문에 정확도는 높지만, 데이터의 개수가 무수히 많다면 한 스텝 나아가는데에 시간이 그만큼 오래 걸리게 되는 단점이 있습니다. 전체 데이터를 (Full) Batch 라는 단위로 정의할 때가 많기 때문에 Batch Gradient Descent 라고도 합니다. 가중치 $\theta$ 에 대해서, GD 를 이용한 최적화는 다음 식과 같은 형태로 정의됐습니다. 

$$
\begin{equation}
\theta = \theta - \eta \cdot \nabla_\theta J(\theta)
\end{equation}
$$ 

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/full-batch.png" />
<figcaption> Batch Gradient Descent </figcaption>
</figure>
</center>

# 확률적 경사하강법 (Stochastic Gradient Descent, SGD)
현실적인 측면에서 바라볼 때, GD 를 적용하기에 어려움이 있기 때문에 SGD 가 등장합니다. SGD 의 기본 개념은 `모두 다 보는게 아니라, 적당적당히 살펴보자` 입니다. 정확히는 전체 데이터중에서 무작위로 `한 개` 의 데이터를 뽑아서 최적화하는 방법입니다. 전체를 다 보는게 아니라 하나씩만 고려하기 때문에 속도가 엄청 빠른 대신, 정확도가 떨어집니다. $x^{(i)}, y^{(i)}$ 를 이용한 $\theta$ 에 대한 업데이트를 다음과 같이 나타낼 수 있습니다.

$$ 
\begin{equation}
\theta = \theta - \eta \cdot \nabla_\theta J(\theta;x^{(i)};y^{(i)})
\end{equation}
$$ 

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/sgd.png" />
<figcaption> Stochastic Gradient Descent </figcaption>
</figure>
</center>

# 미니배치 경사하강법 (Mini-batch Gradient Descent)
GD 와 SGD 는 너무 극단적입니다. 한개씩 보거나 전체를 다 보거나, 선택지가 이 두가지 뿐이라면 너무나 슬플것입니다. 그래서 우리는 SGD 보다는 많이, 그러나 GD 보다는 적은 데이터를 보면서 최적화를 하는 중도를 택하게 되는데, 그것이 미니배치입니다. 전체 데이터를 full-batch 로 보고, 일정 개수로 이루어진 mini-batch 를 이용해서 학습시키는 것입니다. SGD 와 비슷하지만, $x^{(i)}, y^{(i)}$ 부터 $x^{(i+n)}, y^{(i+n)}$ 까지의 데이터를 사용하게 됩니다.

$$ 
\theta = \theta - \eta \cdot \nabla_\theta J(\theta;x^{(i:i+n)};y^{(i:i+n)})
$$ 

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/mini-batch.png" />
<figcaption> Mini-batch Gradient Descent </figcaption>
</figure>
</center>

> Pytorch 에서 주로 SGD 를 사용하곤 하는데, pytorch 의 SGD 는 DataLoader 에서 batch_size 를 정해주기 때문에 사실상 mini-batch 라고 보시면 됩니다. 

# 모멘텀 (Momentum)
GD 를 사용하다보면, 어느 문제점에 봉착합니다. 앞서 살펴본 글에서는 $W$ 의 값을 임의의 값으로 초기화 하여도 global minimum 에 도달할 수 있었습니다. 하지만, 시그모이드 함수에 대해서 MSE 기법을 적용했을때와 같이, local minimum 이 존재하는 문제에 대해서는 최적화 결과가 global minimum 에 도달할 수 없는 경우가 발생합니다.

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/local-minimum.jpg" />
<figcaption> Local Minimum </figcaption>
</figure>
</center>

여기서 모멘텀의 개념을 도입하게 됩니다. 모멘텀 최적화 기법은 현실의 물리에서 관성의 개념을 따왔습니다. 최적화를 공이 굴러가는 것에 비유해보면, 공은 local minimum 에 도달하여도 갑자기 속력이 0 이 되거나 하지 않습니다. 이러한 관성을 최적화에 도입하게 되면, 최적화 과정에서 어느 minimum 값에 도달해도 일단 지나쳐가게 될 것입니다. 이를 점화식으로 나타내면 다음과 같습니다. $\gamma$ 는 모멘텀에 대한 파라미터입니다.

$$
\begin{equation}
\begin{split}
v_{t}  &= \gamma v_{t-1} + \eta \cdot \nabla_\theta J(\theta) \\[5pt]
\theta &= \theta - v_t
\end{split}
\end{equation}
$$

<center>
<figure style="width:80%"> <img src="/Images/Study/mlstudy/5/momentum.jpg" />
<figcaption> Optimization with Momentum </figcaption>
</figure>
</center>

$\gamma$ 가 0 일 경우에는 일반적인 GD 와 동일하게 되며, 일반적으로 0.9 정도의 값을 사용한다고 합니다. 모멘텀을 사용함에 따라 수렴속도도 빨라지고, 왔다갔다 하는 진동도 유의미하게 줄어들게 됩니다.

# 네스테로프 가속 경사 (Nesterov Accelerated Gradient, NAG)


# References
1. <https://www.slideshare.net/yongho/ss-79607172>
2. <https://tykimos.github.io/2017/03/25/Fit_Talk>
3. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.