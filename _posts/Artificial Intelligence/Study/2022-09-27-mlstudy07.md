---
title: "머신러닝 공부 07 - 순전파/역전파"
categories:
  - Study
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true

date: 2022-09-28
---

# 순전파 (Forward Propagation)
다음 알아볼 개념은 `순전파`, 그리고 `역전파` 입니다. 경사하강법을 활용해서 가중치를 업데이트해서 예측값에 대한 오차를 점차 줄여나가는 것이 학습이었습니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/7/forward.png"/>
</figure>
</center>

신경망에서 각각 연산을 수행해 나가며 앞으로 나아가는 것을 `순전파` 라고 합니다. 이 부분은 앞서 살펴본 $H(x)=Wx+b$ 와 같은 연산을 수행하는 것과 동일합니다.

# 역전파 (Backward Propagation)
역전파는 가중치를 업데이트 해주는 부분입니다. 비용함수를 통해 얻어진 오차를 이용해서 $ \theta = \theta - \eta \nabla_\theta J(\theta)$ 와 같이 업데이트를 수행합니다. 

회귀모델을 공부하며, 각 파라미터에 대해 수치미분을 통해 가중치를 업데이트 할 수 있음을 알았습니다. 하지만 ML 분야에서는 `역전파` 혹은 `오차역전파법` 이라는 단어를 사용합니다. 수치미분과 뭐가 다른걸까요?

사실 별거 없습니다. 합성함수의 미분에 대한 성질인 연쇄법칙을 다시 기억해보겠습니다. 아래와 같은 함수를 가정합니다.

$$
\begin{equation}
\begin{split}
z &= t^2 \\
t &= x+y
\end{split}
\end{equation}
$$

여기서 각 변수에 대한 미분은 아래와 같이 이루어집니다.

$$
\begin{align}
\frac{\partial z}{\partial t} = 2t, \quad
\frac{\partial t}{\partial x} = 1, \quad
\frac{\partial t}{\partial y} = 1 \\[10pt]
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial t}\frac{\partial t}{\partial x}= 2t \cdot 1 = 2(x+y) \\[10pt]
\frac{\partial z}{\partial y} = \frac{\partial z}{\partial t}\frac{\partial t}{\partial y} = 2t \cdot 1 = 2(x+y)
\end{align}
$$

이를 신경망에 적용하면 아래 그림과 같이 됩니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/7/backprop.png"/>
</figure>
</center>

즉, 각 가중치들에 대해 수치미분을 하게 되면 모든 수식을 일일이 계산해야하기 때문에 연산량이 무지막지하게 높습니다. 반면, 연쇄법칙을 이용해서 뒤에서부터 하나하나 순차적으로 연쇄법칙을 적용함으로써 간단하게 가중치를 업데이트 해줄 수 있게 되는 것입니다.

[모두를 위한 딥러닝](https://youtu.be/573EZkzfnZ0)의 강의 영상을 보시면 보다 친절하고 자세하게 이해할 수 있습니다.

## References
1. <https://wikidocs.net/60682>
2. [모두를 위한 딥러닝](https://youtube.com/playlist?list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)