---
title: "머신러닝 공부 03 - 소프트맥스 회귀"
categories:
  - Study
tags: [AI, Pytorch, Machine Learning]
toc: true
toc_sticky: true
toc_label: "On this page"
published: true
use_math: true

date: 2022-07-30
last_modified_at: 2022-07-30
---

## 소프트맥스 회귀

선형 회귀와 로지스틱 회귀까지 알아봤습니다. 로지스틱 회귀를 이용해서 Yes/No, True/False, 강아지/고양이 와 같이 두 개의 라벨로 나눌 수 있는 이진 분류 (Binary Classification) 문제를 풀 수 있었습니다. 하지만 강아지와 고양이에 더해서 기린, 호랑이 등 여러 라벨을 가지는 문제에 대해서는 로지스틱 회귀로 풀기 어려웠습니다. 이렇게 세 개 이상의 선택지로부터의 분류 문제를 풀기 위해서는 `소프트맥스 회귀 (Softmax Regression)` 을 이용하게 됩니다. 소프트맥스 회귀는 $$k$$ 차원의 데이터로부터 $$k$$ 개의 각 클래스에 대한 확률을 추정합니다. a, b, c 라는 세 개의 클래스가 있다고 했을때, 이에 대해서 softmax regression을 적용하면 다음과 같습니다.

$$
\begin{equation}
p_i = \frac{e^{z_i}}{\sum^k_{j=1}e^{z_j}}
\end{equation}
$$

$$
\begin{equation}
softmax(z) = \left[ p_1, p_2, p_3 \right] = \left[ p_{a}, p_{b}, p_{c} \right]
\end{equation}
$$

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/3/softmax.jpg" alt=""/>
<figcaption>Softmax Regression</figcaption>
</figure>
</center>

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/3/matrix.jpg" alt=""/>
<figcaption>Matrix 표현</figcaption>
</figure>
</center>

입력 차원이 5개인 데이터에 대해서는, softmax 함수의 input 으로 사용하기 위해서, $$z$$ 차원수를 맞춰 가중치 곱을 수행합니다. 이렇게 해서 얻어진 $$z$$ 를 이용해서 softmax 함수를 거치면 최종적으로 각 클래스에 대한 확률이 나오게 됩니다.

## 손실 함수

로지스틱 회귀에서는 0~1 사이의 확률로, 0.5를 기준으로 이진 분류를 수행했습니다. 이번엔 클래스가 세 개가 되었는데, 이 경우에는 어떻게 해야 손실값을 구할 수 있을까요? 먼저 a, b, c 클래스를 `원-핫 인코딩 (One-Hot Encoding)` 기법을 통해 변환을 해주겠습니다. 원-핫 인코딩은 각각의 클래스를 벡터로 만들어주는 기법으로, 그림으로 나타내면 알기 쉽습니다.

<center>
<figure style="width:50%"> <img src="/Images/Study/mlstudy/3/one-hot.jpg" alt=""/>
<figcaption>One-Hot Encoding</figcaption>
</figure>
</center>

이렇게 원-핫 벡터로 나타내게 되면, 로지스틱 회귀에서 수행했던 것처럼 손실값을 구할 수 있습니다. 그리고 이 손실값을 각 클래스에 대해서 모두 구해서 더해주면 되는데, 이 손실 함수를 `크로스 엔트로피 (Cross Entropy) 함수` 라고 합니다. 실제값 $$j$$ 번째 인덱스의 클래스 $$y_j$$ 에 대해, 샘플 데이터가 $$j$$ 클래스일 확률을 $$p_j$$ 로 정의합니다.

$$
\begin{equation}
\mathcal{L}(W) = -\sum_{j=1}^{k}y_{j}\ log(p_{j})
\end{equation}
$$

앞서 살펴보았던 로지스틱 회귀의 손실 함수를 다시 살펴보면, $$\mathcal{L}(W) = -\left[ylog(H(x))- (1-y)log(1-H(x)) \right]$$ 와 같습니다. 이 식에서 $$ y=y_1, H(X)=p_1, y-1=y_2, 1-H(X)=p_2 $$ 로 바꿔쓰면 다음과 같이 나타낼 수 있습니다. 즉, 로지스틱 회귀의 손실 함수는 클래스의 개수가 크로스 엔트로피 함수와 동일한 함수식임을 의미합니다.

$$
\begin{align}
\begin{split}
\mathcal{L}(W) &= -\left[y_1log(p_1)+y_2log(p_2) \right] \\
 &= -\sum_{j=1}^{2}y_{j}\ log(p_{j})
 \end{split}
\end{align}
$$

위 식은 한 개의 샘플 데이터에 대한 손실 함수로, $$n$$ 개의 데이터에 대한 전체손실(비용)을 계산하는 경우의 식은 다음과 같이 정리됩니다.

$$
\begin{equation}
C(W) = -\frac{1}{n}\sum_{i=1}^{n} \sum_{j=1}^{k}y^{(i)}_{j}\log(p^{(i)}_{j})
\end{equation}
$$

## Pytorch 에서 구현하기

Pytorch 를 이용하여 위에서 살펴본 비용함수를 로우레벨에서 구현하는 방법부터 내부 연산을 모두 포함한 메소드까지 순차적으로 작성되었습니다. `F.cross_entrop()` 함수는 내부에 softmax 함수까지 포함하고 있습니다.
```python
import torch
import torch.nn.functional as F

torch.manual_seed(1)

z = torch.rand(3, 5, requires_grad=True)
hypothesis = F.softmax(z, dim=1)

y = torch.randint(5, (3,)).long()  # tensor([0, 2, 1])
y_one_hot = torch.zeros_like(hypothesis)
y_one_hot.scatter_(1, y.unsqueeze(1), 1)  # in-place operation
"""y_one_hot
tensor([[1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.]])"""


# a - low level cost assumption
cost_a = (y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()
print(cost_a)

# b - F.softmax() + torch.log() = F.log_softmax()
cost_b = (y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()
print(cost_b)

# c - F.log_softmax() + F.nll_loss() = F.cross_entropy()
cost_c = F.nll_loss(F.log_softmax(z, dim=1), y)
print(cost_c)

# d - F.cross_entropy()
cost_d = F.cross_entropy(z, y)
print(cost_d)
```

```bash
tensor(1.4689, grad_fn=<MeanBackward0>)
tensor(1.4689, grad_fn=<MeanBackward0>)
tensor(1.4689, grad_fn=<NllLossBackward0>)
tensor(1.4689, grad_fn=<NllLossBackward0>)
```